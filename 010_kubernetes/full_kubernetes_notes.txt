Kubernetes is a tool that helps us to run and manage applications in containers. It is an open-source container orchestration platform that automates the deployment, management, and scaling of container-based applications in different kinds of environments like physical, virtual, and cloud-native computing foundations.

Kubernetes Cluster Architecture (With Multiple Masters and Workers)
 Worker → Master: Uses Load Balancer to distribute API requests.
 Master → Worker: Communicates directly without a Load Balancer.
	•  The Master already knows each Worker’s IP. 
•  It needs full control over Worker Nodes. 
•  kubelet and kube-apiserver handle direct requests efficiently
4 components of master node:
	•  kube-apiserver 
•  kube-scheduler 
•  kube-controller-manager 
•  etcd cluster
API server (REST language talks only)
•  kubectl is a client that converts commands into REST API requests. 
•  Requests are sent to the API Server, which handles everything, sents keys too . 
•  By default, the API Server performs 4 actions: 
1.	Authenticate – Checks who is making the request.
2.	Authorize – Verifies if the user has permission.
3.	Validate – Ensures the request data is correct.
4.	Talk to etcd – Stores or retrieves cluster state data.
ETCD :  No sql Database designed to scale horizontally.
•  API Server, Scheduler, and Controller Manager do not store data. 
•  Only ETCD stores data because it is the key-value store that maintains the cluster state.

What can API Server do with Database?
Ans: Querry
	  CRUD operation , list /select
POD : bigger than container.
When you create, modify, or delete a Pod, it's like performing operations on a database table stored in etcd.
•	Create a Pod → Insert a new entry in the Pod table (stored in etcd).
•	Modify a Pod → Update the existing entry in the Pod table.
•	Delete a Pod → Delete the entry from the Pod table.

In  kubernetic, api server not the only one do all operations:
The API Server is like a receptionist in a hotel—it doesn’t do everything itself but acts as the central point for all requests.
API SERVER : CRUD operation with ETCD.

SCHEDULAR
The moment when we do entry in POD table, we don’t know which machine it is ,schedular picks automatically and run one of the algorithm and finds out the best machine.

•	ETCD : will have all info …cpu info,memory info
•	Every master have different ETCD
•	API server can only directly talk to ETCD
•	Whoevr needs to talk to ETCD comes to API server and api server talks to ETCD

    CONTROLLER


Kubelet:
o	Continuously sent info about worker node to master
o	If any container downs replace container
o	Intimates to api server
	



HOW DOES ETCD GETS ALL INFO OF MEMORY AND CPU?
Through  kubelet. 

POD :
o	Group of 1 or more container. 
o	You can run multiple containers in a pod.
o	Shared namespace
o	All containers inside pod are collocated,co-scheduled.
o	Pod will be having 1 ip and containers inside will also have same ip

•	API Server, etcd, Scheduler, and Controller Manager are NOT daemon processes because:
o	They are running as pods.
•	Pods : independent and alive but they go together
o	Eg:  pod of peace
•	Recommendation: Run only 1 container in pod
•	When to Use Multiple Containers in a Pod?
🔹 When containers must work together (e.g., a web server + sidecar logging container).
🔹 When they share storage and need to communicate using localhost.
•	Pod crashes: only front end crashes but need to do entire thing.
•	All of containers will run on same pod, all the containers in the pod are collocated and co-scheduled(coming together), share spaces. 
•	Multiple containers in pod share name spaces.
•	The pod has a single IP address (not separate IPs for each container).
o	All containers inside pod having asame ip of pod,if any crashes only kuberenets will replace container(maybe diff ip)
•	How to access 1 container? 
How to Access a Specific Container in a Pod?
1.	From Within the Pod (Using localhost)
Since containers share the network namespace, they can talk to each other using localhost:<port>
•	attach a volume in pod,that voulme is available to be mounted on all the containers.
•	How to Attach a Volume in a Pod?
In Kubernetes, when you attach a volume to a Pod, it can be shared across all containers in the Pod because all containers in a Pod share the same storage namespace.
•	Why Can All Containers Access the Volume?
1️. Pods Share Storage Namespace → Any volume attached to a Pod is available for all containers inside it.
2️. Containers Can Mount the Same Volume Path → Multiple containers in the same pod can mount the volume at different or the same paths.
3️. Useful for Data Sharing → Example: One container writes logs, another container reads and processes them.


•	2 containers of diff pods can talk to each other using ip address.
•	What happens when u create a pod?
o	Kubectl makes rest api req to api server(passes certificate,actual req(get,put,post,dlte),access key & secret key.
o	Api server:
	Authenticate(using certificate) -> ap server talk to etcd
	Autherize -> 
	Validate -> creating pod
•	Store data in etcd(not in tables)
•	Kubelet run abhipod –image-=nginx

Slno
Podname = abhipod
Image = nginx
Node (scheduler finds) = workernode
Desired status->running(controller)
Actual status(controller)
Kubectl->Api server -> kubelet -> CRI
Kubelet work with cri
•	When a pod crashes, replace with new pod (ip address diff- no guarantee having same ip),new named spaces.
•	Pods are ephemeral
•	What Kubelet Actually Does?
•	Kubelet does NOT create containers directly. Instead, it communicates with the Container Runtime Interface (CRI) to manage and run containers.


POD CREATION:

kubectl → API Server → etcd → Scheduler → API Server → Kubelet → CRI → Containers.

AWS:

Kubectl describe pod podname
Kubectl  describe node node_name  | grep “Taint”

To remove taint
Kubectl taint node node_name  <taint>--
	Untainted
kubectl taint node kmaster node-role.kubernetes.io/control-plane:NoSchedule-
kubectl get pod


installing kubernetics:
		kubeadm – tool used to set up a Kubernetes cluster easily
		kubecops
		kindinstall – within docker you can create
		hardway of installation
		
kubectl – communicate with API servers
kubeadm reset
kubeadm init  -fail sometimes-installing kubernetics
kubeadm init –ignore -preflight-errors
kubelet – interact with cri and creates pod

Creating pod:
	kubectl run mypod –image=nginx
kubectl get pod -o wide (where it get created) – randomly went to one of those machines.`
curl <pod ip> = pod running on other machine and accessing in diff machine using ip(nating happens).(worker – ( pod created )to master).
Kubectl api-resources(all resources)
Short form for pod -po
Ps -ef | grep runc (no containerd)
Kubectl delete pod –all ( don’t change systems pod)
Kubectl get po -n kube-system -o wide ( can see 2 podes-proxy,calico)

Create a pod -> see which which machine running -> go to the node -> do ps-ef | grep runc -> see the containers -> kill it -> again ps -ef | grep runc -> container will be there -> kuberentes replace the container

	
Specification:
	Networking interface of kubernetics.
	JVM

PAUSE CONTAINER:
	If we are creating docker container use pause containers, 
	Automatically inject a container.
	When docker is making any change they are talking to container pod.
	If pause container crashes , entire pod will be replaced
	If our container crashes,it only replaced.
	
Purpose: docker made a docker container to communicate with Kubernetes instead of  directly communicating with docker.


When u crash pod what happens?
	Pulling image
	Created container
	Started pod
o	Locally fixed by kubelet
If I dlte the pod my mistake.Is there any way to solve this?
Kubelets and cri = not run as a pod
No swapspace utilization in Kubernetes
	Sometimes  crash happens  when the process in harddisk comes back and active.
	Sudo swapoff -a -> swap off in current session
Restart -automatically disabled by other command(for future session)
CREATING POD USING YAML FILE
o	Cloning the repo
o	Cd docker-k8s/yaml/calico
o	kubelet
o	Ls
o	Cd ..
o	Cd depoloyment
o	Cat ng-pod.yaml 

	Metadata -> we can mention anythg
	Try to work with the obj of its name
	One pod with unique name, otherwise fails
	Containers:
•	Name:nginx(reference)-doesn’t mean kubernetses will create a container
Some imp commands:
Kubectl apply -f  file_name(create pod)
Kubectl delete -f (dlte all obj present in that fle)
Kubectl dlte podname (dlte pod)
Kubectl get object object_name
Kubectl describe object object_name
Kubectl api-resources
Delete pod –all
Kubectl edit rs <frontend> = directly make changes in running objects without tracability

Kubectl get secrets -n kube-system
Kubectl get ns(namespaces) =4
What all are running inside kubesystem namespace?
	The kube-system namespace is created by default in every Kubernetes cluster. It stores essential system components needed for cluster operations.
🔹 What is patch in Kubernetes?
In Kubernetes, a patch is used to update an existing resource partially (without deleting and recreating it).
🔹 Why Use Patch?
✅ Modify only specific fields instead of replacing the entire YAML
✅ Faster than kubectl apply
✅ Useful for quick updates like changing labels, images, or annotations

🔹 Kubernetes Replicas, ReplicaSets, and Deployments - Simple Explanation
     	📌 What is a ReplicaSet?
A ReplicaSet ensures that a specific number of identical Pods are running at all times. If a pod fails, the ReplicaSet creates a new one automatically.
     📌 Commands to Manage ReplicaSets
1️⃣ Check existing ReplicaSets
kubectl get rs
2️⃣ Scale a ReplicaSet to 3 pods
kubectl scale rs frontend --replicas=3
3️⃣ Check pod details (IPs will be different)
kubectl get pods -o wide
4️⃣ Delete a pod (A new pod will be created automatically)
kubectl delete pod <pod-name>
💡 Pods are not guaranteed to have the same IP after deletion!
________________________________________
🔹 Deployment → ReplicaSet → Pods
A Deployment manages ReplicaSets, which in turn manage Pods.
✅ Advantages of Deployments
✔ Scaling (Scale Out & Scale In)
✔ Self-healing (Pods restart if they fail)
✔ Rolling Updates & Rollbacks (Upgrade safely)
✔ Automatic failover (Pods restart automatically)
✔ Zero Downtime Upgrades (Ensures app is always available)
________________________________________
🔹 Rolling Updates (Zero Downtime Upgrade)
Instead of stopping everything at once, Rolling Updates update pods one by one without downtime.
1️⃣ Upgrade Deployment (Edit the YAML or use command)
kubectl set image deploy nginx-deployment nginx=nginx:1.21.0
2️⃣ Check rollout status
kubectl rollout status deploy nginx-deployment
________________________________________
🔹 Rollback (Undo an Update)
If the new update fails, rollback to the previous version:
kubectl rollout undo deploy nginx-deployment
________________________________________
🔹 Summary
•	Deployment manages ReplicaSets, which manage Pods.
•	ReplicaSet ensures a fixed number of pods are running.
•	Rolling Updates ensure zero downtime.
•	Rollback restores the previous stable version if needed.
💡 Deployments provide easy scaling, updates, and self-healing for applications! 🚀
🔹 What is a Label in Kubernetes?
A label is a key-value pair used to categorize and organize Kubernetes resources like Pods, Deployments, and Services.
	apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    env: production   # 👈 Label key = env, value = production
    app: my-app       # 👈 Label key = app, value = my-app
spec:
  containers:
    - name: nginx-container
      image: nginx
🔹 Why Use Labels?
✅ Identify and filter resources easily
✅ Group similar resources (e.g., all env=production Pods)
✅ Selectors use labels to find matching resources
✅ Useful for scaling, rolling updates, and networking
🔹 Get Resources Using Labels
1️⃣ List Pods with a specific label
kubectl get pods -l app=my-app
2️⃣ Delete all Pods with a specific label
kubectl delete pods -l env=production
3️⃣ Apply a label to an existing resource
kubectl label pod my-pod tier=frontend

Why Are There Extra Characters in Deployment Name?
When you create a Deployment, Kubernetes automatically generates a ReplicaSet with a random suffix to ensure uniqueness. This happens because:
1️⃣ Deployments create ReplicaSets, not just Pods
2️⃣ ReplicaSets use a hash suffix (e.g., nginx-deployment-8d94c585f)
3️⃣ Pods inherit the ReplicaSet name with their own unique suffix
Q. When Does a Deployment Create Multiple ReplicaSets?
o	Updates and roll back 
o	Deployment controller make its entry
o	Replication controller makes its entry

Q. Create a completely new spring boot application and push the image and update and deployment..
Q. why apps/v1
Q. Always restapi will have versions y?
o	comeup with other endpoints later v2/kll
o	after coming new versions only remove older one
Q. Service discovery and load balancing:
o	Service discovery allows services to find and communicate with each other without knowing IP addresses.
o	ClusterIP is the default service type in Kubernetes that allows communication between Pods inside the cluster. It provides service discovery and load balancing automatically.

How clusterip works?
1.	Imagine each container on different pods (p1 and p2)
2.	Each container inside pod is listening to same service(c1 and c2)
3.	If some other container(c3) of diff pod(p3) within the cluster need to access the service listened by the container mentioned in the p1 or p2 (same service)
4.	P3 goes to clusterIP (static ip) and clusterip decides the pod.
5.	ClusterIP acts as a load balancer and decides which Pod (p1 or p2) should receive the request.
6.	By default, it follows a round-robin strategy.
7.	The request reaches one of the available Pods dynamically.

o	Nordport: make services accessible to the trusted service within the cluster(my own babies).
o	Nodeport always create cluster ip internally.
o	Nodeport reserves port on hostmachine 
o	Loadbalancer: Used in cloud providers like AWS, GCP, Azure.
 Exposes a public IP for external access.
o	Target pod: The Pod that receives the request when you access an application is called the Target Pod.

        
Q. How to Find Pods Associated with a Service in Kubernetes?
        A Kubernetes Service selects Pods using label selectors.

15/3/25
raft algorithm?
o	finds a node which is doings things faster as master
o	if master goes down again same raft algorithm comes into picture
daemon sets?
o	created on all worker nodes
o	1 pod in all nodes(use-cases:monitoring,kubeproxy,logging agents)
statefull set?
o	also kubernetes service created along with replica set
o	multiple rEplicas,kubernetes service forward req to primary then secondary if primary crashes
o	3 pods -> req will go to pod to pod if that pod crashhes..done by load balancer
o	which pod comes up first is primary
o	cluster ip acts as load balancers

replication controller and replicaset?
o	replica set: in check 
o	replication controller: equals











creation of pod:
/.kube/config-context -certificate- signed by kubernetes client->kubectl(restapi req)->info go to header
master -control plane
worker-data plane 
certfile,keyfile - authentication
roles,rolebindings,clusterrole and binding - autherization(data in etcd)
api - event driven architecture
scheduler - finds best node to ocreate a pod,scheduler will get triggered(etcd -how much info there in cpu(worker))
controller - create a pod,manages the pod   
                replica,node,pod,deployment,job controller -> packaged into single


worker:
proxy : knows everythg about networking
        ip
        pods associated with the ip
etcd : no sql ,data comes to etcd
        statefull - persist -scale horizontally

scheudler : picks the node
controller comes into picture when actual state not equal to current
then goes to api server and to kubelet work with container d creates pod..come back to api server then to etcd -> running
kubelet knows how much menory is there..updates the etcd.



crud operation happens b/w api server and etcd
scheduler - 

creating a pod wht happens:
        pod table:
        slno     name      image    curret-state     desired state   node
        1       ustpod       ngninx                     running         

        kubectl to api server-create pod
---------------------------------------------------------------
replica set created:
        rs table :
                sno 
                name 
                desired replicas
                actual replicas
                image name
kubectl looks current context ( get certificate and key)
convert rest api req and sents -> api server after 4 sets makes an entry
checks desired and actual -> replication controller comes -> create pods
tABLE:                                                                                          
        sno 
        name 
        image
        desired
        actual

        schedueler -> picks the node
        controller -> creates the pod
        cri

if new entry came,scheduler will come
node goes down -> kubelett api sever communicate
replicaset-
        self healing
        scale in,out


-----------------------------------------------------------------

DEPLOYMENT:
        Sl no 
        name 
        current state
        desired
        image 

        deployment controller comes into picture -> create replica set
        replication controller watches and make entry in pod table
        scheduler watches -> update the machine where created
        controller - 
        api server talks to kubet and created

rolling update(n num of pods will come up once active) and roll back 
        other staretegies support by kubernetics apart from rolling update->
                                  recreate(deployment strategy)- remove all pods and create all set of pods
update :
        new entry in replicaset table
        increase count of that and reduce count of old one in replica set and new pod of this verrsion created and the countb of replica set oledr entry reduces
        look ppt  
        kubectl set image deploy deployment name nginx=nginx:1.16.1








create deployment having our image name
Create a completely new spring boot application and push the image and update and deployment..
why apps/v1
always restapi will have versions y?
    comeup with other endpoints later v2/kll
    after coming new vwrsions only remove older one
service discovery and load balancing:
How to Find Pods Associated with a Service in Kubernetes?
        A Kubernetes Service selects Pods using label selectors.

15/3/25
raft algorithm?
        finds a node which is doings things faster as master
        if master goes down again same raft algorithm comes into picture
daemon sets?
        created on all worker nodes
        1 pod in all nodes(use-cases:monitoring,kubeproxy,logging agents)
statefull set?
        helps in active passive deployment
        used it in databse 
        always talk to active
        redundancy:
                active active : req sent to any of them(stateless)
                active passive  deployment:  1 node is backup -passive
                                        always talk to active -(consistency)
                                        if active crashes passive will take the responsibility
        also kubernetes service created along with replica set
        multiple rEplicas,kubernetes service forward req to primary then secondary if primary crashes
        3 pods -> req will go to pod to pod if that pod crashhes..done by load balancer
        which pod comes up first is primary
        cluster ip acts as load balancers

replication controller and replicaset?
    replica set: in check 
    replication controller: equals

network policy?
named space 

give same label to multiple namespace
namespace with label
podselector: {} -> all pods in 1 namespace

17/3/25
INGRESS: 
    creating diff path for diff service
    pathway load balancing using ingress
    kubernetes ingress object 
    kubernetes load balancer -> create application load balancer 
 docker/yaml/ingress/simple/instruction.txt
                created deployment
                cluster ip service
                ingress 
                when u run the ingress controller ,automatically create a nodeport..create ingress rules and forward acc to that service
                when ever u hit the service -> getting load balancer of path based and instance based
                path : /  forwarded to service
                no -o wide give private ip
                kubectl get po --all-namespaces -l <selector>

kubeadm init -> what all it does?(installation)
adm.conf:
        have all certificate details,permissions, -> sent to server

static pod: 
        manifests
        created along with scheduler,api server,controller
        dwld all yaml file and put on that folder
        object name-machine name
        changes-> automatically apply
        dlted-> restart
        after moving yaml file /home/master -> it will delete the static pod
dry run :
        will not try to create

skipping phase not req?
        didnt give any parameter while init -> req when using ext load balancer(address of lb),upload certificates
        when multiple master node using its required.

 What is /etc/kubernetes/kubelet.conf?
        /etc/kubernetes/kubelet.conf is the configuration file for the Kubelet (the node agent in Kubernetes). This file contains details about how the Kubelet communicates with the API server and manages workloads on a node.

Creating the cluster-info ConfigMap in Kubernetes
        The cluster-info ConfigMap is used by kubeadm for bootstrapping nodes in a Kubernetes cluster. It stores essential cluster-related information such as the API server endpoint and certificates.
        kubectl get cm -n kube-public,  describe
        6443 - api server port number

cat /etc/kubernetes/manifests/kube-apiserver.yaml
        The kube-apiserver.yaml file is the static pod manifest that defines the Kubernetes API server. This file is managed by Kubelet and is responsible for starting the API server inside a pod on the control plane node.

10250 - kubelet




## **💡 What is ConfigMap & Secret?**  
Think of **ConfigMap** as a **settings file** (like an `.ini` or `.env` file) that stores **non-sensitive** information, such as:  
- Database hostnames  
- Port numbers  
- Log levels  

A **Secret** is like a **password manager** that stores **sensitive** information, such as:  
- Passwords  
- API keys  
- Certificates  

---

## **🟢 1. How to Use ConfigMap?**  
We can inject **ConfigMap** into a pod in **two ways:**  
1. **As Environment Variables** (Static values)  
2. **As a Volume** (Reflects changes automatically)  

### **📌 1️⃣ Using ConfigMap as Environment Variables**
This method is like using `export VAR=value` in Linux.

#### **🔹 Step 1: Create a ConfigMap**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config  # Name of the ConfigMap
data:
  DB_HOST: "mysql.database"  # Storing database hostname
  DB_PORT: "3306"  # Storing database port number
```

#### **🔹 Step 2: Inject ConfigMap into a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      env:
        - name: DB_HOST  # Environment variable inside the pod
          valueFrom:
            configMapKeyRef:
              name: my-config  # Referring to ConfigMap we created above
              key: DB_HOST  # Fetching DB_HOST value from ConfigMap
```

✅ **What Happens?**  
- Inside the pod, you can access the value using:  
  ```bash
  echo $DB_HOST  # Output: mysql.database
  ```
❌ **Issue:** If the ConfigMap changes, the pod **won't** get updated unless restarted.

---

### **📌 2️⃣ Using ConfigMap as a Volume**
This method mounts the ConfigMap as **a file inside the pod**, so any changes in the ConfigMap **automatically** reflect in the pod.

#### **🔹 Step 1: Create a ConfigMap**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config  # Name of the ConfigMap
data:
  db-config: |
    DB_HOST=mysql.database
    DB_PORT=3306
```

#### **🔹 Step 2: Mount ConfigMap as a Volume in a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      volumeMounts:
        - name: config-volume
          mountPath: "/etc/config"  # Where ConfigMap will be mounted inside pod
  volumes:
    - name: config-volume
      configMap:
        name: my-config  # Referencing the ConfigMap
```

✅ **What Happens?**  
- Inside the pod, a file `/etc/config/db-config` is created with:  
  ```bash
  cat /etc/config/db-config
  # Output:
  # DB_HOST=mysql.database
  # DB_PORT=3306
  ```
- **If the ConfigMap is updated, the pod gets the changes immediately.**

---

## **🔴 2. How to Use Secrets?**
Just like ConfigMap, we can inject **Secrets** using **environment variables** or **volumes**.

### **📌 1️⃣ Using Secrets as Environment Variables**
#### **🔹 Step 1: Create a Secret**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-secret  # Name of the Secret
data:
  DB_PASSWORD: bXlzZWNyZXQ=  # "mysecret" encoded in Base64
```
> 🔹 To encode a value manually, use:
> ```bash
> echo -n "mysecret" | base64
> ```

#### **🔹 Step 2: Inject Secret into a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      env:
        - name: DB_PASSWORD  # Environment variable inside the pod
          valueFrom:
            secretKeyRef:
              name: my-secret  # Referring to the Secret
              key: DB_PASSWORD  # Fetching value from Secret
```

✅ **What Happens?**  
- Inside the pod, you can access the value using:  
  ```bash
  echo $DB_PASSWORD  # Output: mysecret
  ```
❌ **Issue:** If the Secret changes, the pod **won't** get updated unless restarted.

---

### **📌 2️⃣ Using Secrets as a Volume**
This method mounts the Secret as **a file inside the pod**, so any changes in the Secret **automatically** reflect in the pod.

#### **🔹 Step 1: Mount Secret as a Volume in a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      volumeMounts:
        - name: secret-volume
          mountPath: "/etc/secret"  # Where Secret will be mounted inside pod
  volumes:
    - name: secret-volume
      secret:
        secretName: my-secret  # Referencing the Secret
```

✅ **What Happens?**  
- Inside the pod, a file `/etc/secret/DB_PASSWORD` is created with the secret value:  
  ```bash
  cat /etc/secret/DB_PASSWORD
  # Output:
  # mysecret
  ```
- **If the Secret is updated, the pod gets the changes immediately.**

---

## **🛡️ 3. How to Encrypt Secrets in etcd?**
By default, **Secrets are only Base64-encoded, not encrypted**. We can encrypt them **before storing in etcd**.

#### **🔹 Encryption Configuration**
```yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources: ["secrets"]
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: c3VwZXJzZWNyZXQK  # Base64-encoded encryption key
      - identity: {}
```
After applying this, **Kubernetes will encrypt Secrets** before storing them in etcd.

---

## **🎯 Summary**
| Feature | ConfigMap (📜) | Secret (🔐) |
|---------|--------------|-------------|
| Purpose | Store settings (non-sensitive) | Store passwords, API keys (sensitive) |
| Example Data | `DB_HOST=mysql.server.com` | `DB_PASSWORD=MySecret123` |
| Security | Plain text | **Base64-encoded** (not fully encrypted) |
| Access Control | Less restricted | **More restricted (RBAC)** |
| Encryption | Not encrypted by default | Can be encrypted in etcd |

✔ **ConfigMap** = Non-sensitive settings (like DB host)  
✔ **Secret** = Sensitive data (like passwords)  
✔ **Use Environment Variables** for **static** values  
✔ **Use Volumes** for **dynamic** updates  
✔ **Encrypt Secrets in etcd** for better security  

---

### **🤔 Still Confused?**
Let me know **which part** you didn't understand, and I'll **simplify it even more**! 😊




sir code:


pod-with-config-map.
        apiVersion: v1  # API version used for creating a Kubernetes Pod.
        kind: Pod       # Specifies that this is a Pod definition.
        metadata:
        name: example-pod  # Name of the pod.

        spec:
        containers:
        - name: example-container  # Name of the container inside the pod.
            image: nginx  # Using the official Nginx image.

            env:  # Setting environment variables inside the container.
            - name: EXAMPLE_ENV_VAR  # Name of the environment variable.
            valueFrom:  # Getting value from an external ConfigMap.
                configMapKeyRef:
                name: example-configmap  # Name of the ConfigMap to reference.
                key: example-key  # The specific key inside the ConfigMap.


What This Does?
Creates a Pod named example-pod.
Runs an Nginx container inside that Pod.
Injects a value from ConfigMap into an environment variable inside the container.
The ConfigMap (example-configmap) has a key example-key with a value (example-value).
This value is assigned to an environment variable (EXAMPLE_ENV_VAR) inside the container.
The container can access this value as an environment variable.








18/3/25
2 containers-2/2

scaling:
        hpa- Horizontal Pod Autoscaler
        vpa- Vertical Pod Autoscaler 
        ca -cluster autoscaling

hpa -   automatically adjusts the number of pods in a deployment based on resource usage.
        if resource usage increases adding more replicas(pods)
        traffic->decrease-> remove pods
        min replica,max replica count
        if cpu utilization >50 ,add more pods
        more replica-> performance more
        horitontal is better

        1. create deployment with cpu limit:  
                                        soft limit -req lmit
                                        hard limit (500) - kill go beyond
        2. create hpa and kubernetes increases If
                                        goes beyond limit
        more load.. what the pod can handle?
                limit is bigger than vpa
                cannot maually set the replicas=....
                how can u automatically increase/
                        put resource limit on pod
                        otherwise->memory leak
                        memory leak in the pod(beyond the limit utilization)
                        all the other services in the pod got impacted -> because no enough resources
metrix-server: hpa use this to see the cpu all utilization
vpa-    VPA adjusts the resource requests and limits of individual pods rather than 
        adding more replicas.
        if an app -> running on 1 pod-> uses more memory than expected-> wont add pods -> add  resource limits of the existing pod 
        define criteria where u can add more pods
        memory leak in the pod(beyond the limit utilization)
        all the other services in the pod got impacted -> because no enough resources
        create deployment
        create service : from 1 pod talking to other using name of the service 
        so pod getting load beyond the limit

cluster - add more machines(worker nodes) ->check enough capacity for a pod to run
        kubernetes need to work with underlying provider
        If pending pods cannot be scheduled due to insufficient resources, CA provisions new nodes
        If nodes remain underutilized for an extended period, CA removes them

CRD: custom resource Definition
        every resource have controller
https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/hpa/Notes.txt

managed kubernetes cluster : managed by cloud provider(gke)

1000m - 1 cpu
200m - 20 % of 1 cpu
limit - 500m (beyond- kill)
targetcpuutilizationpercentage = 50 (50%of limit) i.e,250m

self healing?
        create replicaset
        deployment
        daemon set
        stateful set

SCHEDULING:
        based on 
                nodename(specific node)
                nodelabel(give same label to diff node,group of node)
                        node is taint: stain(no pod will go there)
                                       existing pod?
                                       create a pod which can tolerate the taint(doesnt meant it will only go there)
                                       if tolerate-> create another taint which can never tolerate
                                       https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/scheduling/taintsAndTolerations/Notes.txt
                create Daemon set
                Effects of taint
                        taint can produce 
                                3 possible effects:
                                        NoSchedule-> Prevents scheduling new pods unless they have a matching toleration.
                                                        if already pod is running it can run
                                        PreferNoSchedule->  1 node is spcl,other node(not prefer)
                                                            Tries to avoid scheduling pods that don’t have tolerations, but doesn’t strictly prevent it.
                                                            if other machines have constraints then ok to schedule it there
                                        NoExecute-> evicts existing pod which cant tolerate
                                        
                        NoSchedule
                                Kubernetes scheduler 
                                        ALLOW scheduling pods 
                                                with tolerations.
                        PreferNoSchedule
                                Kubernetes scheduler 
                                        TRY TO avoid scheduling pods 
                                                don’t have tolerations.
                        NoExecute
                                Kubernetes 
                                        evict the running pods 
                                                from the nodes 
                                                        if NO tolerations in pods.


https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/scheduling/Notes.txt 35 line
node selector.yaml
        force the label to go to the node by selector
        give label to multiple machines



https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/scheduling/taintsAndTolerations/PodWithTolerations.yaml
apply taint on the node..

key,value-> both match if equals

kubectl get nodes
  131  kubectl taint nodes ip-172-31-39-119 app=DBNode:NoSchedule
  132  k describe ip-172-31-39-119
  133  kubectl describe node ip-172-31-39-119
  134  cd docker-k8s/yaml/scheduling/
  135  kubectl describe node ip-172-31-39-119
  136  ls
  137  kubectl apply -f taintsAndTolerations/PodWithTolerations.yaml 
  138  kubectl get po -o wide

try to create on master node(master have some toleration,create pod with that toleration)
        kubectl describe node <master> | grep -i taint
        add taint of master in that file
        kubectl apply -f taintsAndTolerations/PodWithTolerations.yaml
        kubectl get po -o wide

        if we taint the worker (no guarentee to create on worker,but it can tolerate the taint)
        Master has a taint → Add matching toleration in the pod.
        Worker has a taint → No guarantee, but pods can tolerate it if toleration exists.

affinity:
        liking
        node:
                pod created on that node depends on preffered or required,
                label the nodes and give affinity to nodes
        pod:  
                pod created on the plce where other pods are running.
                if the pod has the label - created on thats
operator = exists means =(key=value:toleartion name)
        check key is there when defining toleration


        required during scheduling
        preffered during scheduling

        managed through labels
node affinity
anti affinity: doesnt want to go
        pod will not created

JOBS- asynchronous
cronjob
why should i use kubernetes cluster jobs instead of cronjob?
        create pods/pod crashes/kubernetes replicaset
        dns of service work from pod not in machines
        running as pods -> controll over who can access
                                         
job:docker-k8s/yaml/job
        one time job: run only once(once in a while in some month),no schedules
        crone job: schedule interval it will work

RBACK: will study about namespaces
ingresss belong to ns 
ingress classes -> 
roll based access controller

*/ = wait until evry minute is started

kubeadm ha availabilty
        kubeadm init address of load balancer
etcd: eventually consitent
        b/w them they talk
        creates copy of data in every ethcd
------------------------------------------------------------------
19/3/25
RBAC:

RBAC in Kubernetes controls who can do what on different resources within the cluster. It is based on Subjects, Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings.
        subject:
                user - kubernetes-admin(admin.conf file),this user have admin privileges,They need permission to do things (e.g., list pods, delete deployments).
                group - for collaboration
                service account  - used by programs,to give access to pod/service
                                   if user create and manage, if he moves other team/all crashed
                                   as long as SA is there ok
        action of subject:
                creating a pod
        installing plugins-> not a part of kubernetes
        ~ -> home directory
        ~/.kube -> config file

        user->dictracy->can change-> permision to list pods->
        give permission to one of the groups in a namespace

        Pod Reader Role → Can only list and view pods.
        Admin Role → Can delete, update, and create resources.

group of clusters will have seperate namespace,consider 2
subject->rolebinding->roles
role->object
rolebinding-> binding subject and roles(assigning roles to users)
in roles,
        have actions(creating pod)
in role binding,
        what actions doing on which object
relationship,
        many to many
cluster role,clusterrole binding:
        A ClusterRole is like giving someone permission across the entire company, not just one department.
        use cluster role binding to bind
        used to giving permission to all namespaces
        not belong to namespace,belong to entire cluster

        ClusterRoleBinding: Assigns ClusterRole to a User/Group in all namespaces
roles and rolebinding:
        associated to namespaces

practical
        clone
        https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/rbac/instructions.txt -142
        1.creates envt variable
        echo $variablename
        2.145 - ls to see -> dictracy.key file created
        3.to get certificate signed->req using key -151 ->csr file
                $magic user->dictracy
                2 o ->organisations
        /etc/kubernetes/pki->certificate and key for CA
        4. sudo openssl x509 -req -in dicktracy.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out dicktracy.crt -days 500
        soo this is actual certificate
        5. now u can onboard to kubernetes -163
        6. ls-a  to see . files
        7.go and check the namespace file
        8.apply and check
        9.kubectl config view -> configurations of cluster
                        server -> ip address of api server
                         @kubernetes-> cluster
        assigning a user to the role in user-pod-reader-rolebinding.yaml
                kubectl get rolebinding -n test

        dictracy users can list the pods in that namespace

        kubectl config use-context kubernetes-admin@kubernetes
        forbidden -> create service acount with proper permission
        context:sessions are maintaineed in context
        config: setting credentials-> adding to context

https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/serviceAccount/instructions.txt

within a pod making an api call -> permission denied
       kubectl describe po nginx-deployment-7fb7fbdd7f-wkd46
  143  kubectl get sa 
  144  kubectl describe sa nginx-sa
  145  kubectl get rolebinding
  146  kubectl describe rolebinding nginx-sa-readonly
  147  kubectl edit rolebinding nginx-sa-readonly
  148  kubectl describe rolebinding nginx-sa-readonly
  149  kubectl exec -it nginx-deployment-7fb7fbdd7f-wkd46 -- /bin/bash


  pod to pod -> network policy,https certificte
  pod to pod -> validate  frm right source or not/


rbac practical:



## **🛠 Step 1: Create a New Namespace**
First, create a namespace where we will test the RBAC setup.  
```bash
kubectl create namespace test
```

---

## **👤 Step 2: Create a User (Without Admin Access)**
For this example, we will create a Kubernetes user **dicktracy**.  

### 🔹 Generate Certificates for User Authentication
Kubernetes uses **TLS certificates** for user authentication. We will create a certificate for `dicktracy`.  

```bash
# Generate a private key
openssl genrsa -out dicktracy.key 2048

# Generate a Certificate Signing Request (CSR)
openssl req -new -key dicktracy.key -out dicktracy.csr -subj "/CN=dicktracy/O=developers"

# Sign the CSR with Kubernetes CA to generate a client certificate
openssl x509 -req -in dicktracy.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out dicktracy.crt -days 365
```
👉 Now, `dicktracy` has a **certificate** but no permissions yet.

---

## **🔗 Step 3: Create a Role to List Pods**
Now, let's create a **Role** that allows a user to **only list pods** inside the `test` namespace.  

```yaml
# Save this file as pod-reader-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: test
  name: pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
```
🚀 **Apply the Role:**
```bash
kubectl apply -f pod-reader-role.yaml
```

---

## **🛠 Step 4: Bind the Role to `dicktracy`**
Now, create a **RoleBinding** to assign the `pod-reader` role to `dicktracy`.

```yaml
# Save this file as pod-reader-rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: test
  name: user-pod-reader
subjects:
  - kind: User
    name: dicktracy  # Assigning permission to this user
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
🚀 **Apply the RoleBinding:**
```bash
kubectl apply -f pod-reader-rolebinding.yaml
```

---

## **👀 Step 5: Verify Permissions for `dicktracy`**
Let's check if `dicktracy` can list pods in the `test` namespace.

```bash
kubectl auth can-i list pods --as=dicktracy -n test
```
👉 If the setup is correct, it should return:
```
yes
```

🚀 **Try listing pods (Should Work)**
```bash
kubectl get pods --as=dicktracy -n test
```

🚨 **Try deleting a pod (Should Fail)**
```bash
kubectl delete pod mypod --as=dicktracy -n test
```
👉 You should get a **permission denied** error.

---

## **🌎 Step 6: Assign Global Permissions (ClusterRole)**
Let's say we want **dicktracy** to list pods in **all namespaces**.

```yaml
# Save this file as cluster-pod-reader.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cluster-pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
```
🚀 **Apply the ClusterRole:**
```bash
kubectl apply -f cluster-pod-reader.yaml
```

Now, create a **ClusterRoleBinding** to assign this ClusterRole to `dicktracy`.

```yaml
# Save this file as cluster-pod-reader-binding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cluster-pod-reader-binding
subjects:
  - kind: User
    name: dicktracy
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io
```
🚀 **Apply the ClusterRoleBinding:**
```bash
kubectl apply -f cluster-pod-reader-binding.yaml
```

✅ **Test the new permissions**
```bash
kubectl get pods --as=dicktracy --all-namespaces
```
👉 Now `dicktracy` can **list pods in all namespaces**!

---

## **🎯 Summary**
| Command | Description |
|---------|------------|
| `kubectl create namespace test` | Create a namespace for testing. |
| `openssl genrsa -out dicktracy.key 2048` | Generate a private key for `dicktracy`. |
| `kubectl apply -f pod-reader-role.yaml` | Create a Role to allow listing pods. |
| `kubectl apply -f pod-reader-rolebinding.yaml` | Bind the Role to `dicktracy`. |
| `kubectl auth can-i list pods --as=dicktracy -n test` | Check if `dicktracy` has permission. |
| `kubectl apply -f cluster-pod-reader.yaml` | Create a ClusterRole to list pods in all namespaces. |
| `kubectl apply -f cluster-pod-reader-binding.yaml` | Bind the ClusterRole to `dicktracy`. |

---

Injecting Configuration into a Pod: ConfigMaps & Secrets
inject property:
        1. configuration ConfigMap- plaintext /stored in etcd after encryption/remains plain text in pod
                         secrets
        2. volume,envt variables

how to inject configmap into pod?
        volume (mounted as files) ->directory -> attach to host and reflect the changes to pod 
        envt variables   ->variable ->static
diff b/w secrets and configmap?
        secrets- encoded
                 using rback we can restrict the access.
                 we have to make it secure
                 dont give access to etcd
                 encrypt data in etcd
                 remains dataa as plain text in pod(limit access)
                 integrate with vaults

/docker-k8s/yaml/secrets
secret got created: create files
                 kubectl create secret generic test --from-file=./username.txt --from-file=./password.txt
other way:
        using yaml file
        manually we need to decrypt
        decrypting:  echo bXlwYXNzd29yZA== | base64 --decode
                     echo YWRtaW4= | base64 --decode
        decrypt the user and password and apply and check the secrets created or not.

         cat secrets-use.yaml  -> mounted the secret to the path /etc/foo
         kubectl apply -f secrets-use.yaml 
         kubectl exec -it mysecret-pod -- /bin/bash
         cd /etc/foo  =inside this folder you can see the password and user files

/etc/kubernetes/manifests# cat kube-apiserver.yaml 
        any file changed in manifests,automatically reapply

cd docker-k8s/yaml/configmap/simple/  -> config name,key value
pod with,.... injected to the contanere of the pod as env
 (EXAMPLE_ENV_VAR=example-value) can see ths in env
------------------------------------------------------------------------
 volume:
        emptyDirectory: Data exists only while the pod is running.
                        same as tempfs in docker(when container dlted,volume also get deleted/stored in ram/if temp data dont want to persist,make writable layer bigger)
                        stored in memory
                        shared by 2 diff container of same pod
                        hostpath- a node inside a pod ,a volume is there (/home/vol)/if pod crashes,kubelet replace pods in any nodes,so data in /home/vol will not be there
                                  not a good idea
                        alternative= in another machine create nfs and data will be available here(network plugin not in kubernetes)
                                     ebs (in aws)

multiple containers talking to eachother in a single pod in empty directory/pod crashes -> its gone

upgrade?
Why Is name Mandatory for Multiple Containers in a Pod?
        there may be 2 containers and we do exec command and entering into a pod..if we didnt mention
        the container name...it can enter into any container, so giving name we can enter to that specific 
        container. (-c container name).

hostpath -> like bind mount
   hostmachines path is mounted with the path inside the pod inside the container /test-mnt
   in hostmachine -> cd /test-mnt -> ls ..get the file created inside the pod 
   while we are inside container and created file,/hostmachine-saw the file/could not have worked y?
                kubernetes having many nodes/volumes may be created in any nodes where pod is getting created


-----------------------------------------------------------------------------
20/3/25


https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/etcdbackup/Notes.txt

etcd backup:
        ETCDCTL_API=3 etcdctl \
		--endpoints=https://127.0.0.1:2379 \
		--cacert=/etc/kubernetes/pki/etcd/ca.crt \
		--cert=/etc/kubernetes/pki/etcd/server.crt \
		--key=/etc/kubernetes/pki/etcd/server.key snapshot save /tmp/snapshot.db  


when u take backup, also give token(parameter) 132
in etcd.yaml -> include that also
-------------------------------------------------------------------------------------
21/3/25

creating older version of kubernetes (31)
upgrade kubeadm to verson 32 ->drain

       sudo apt update

  49 sudo apt-cache madison kubeadm

  50 clear

  51 KUBERNETES_VERSION=1.32

  52 sudo mkdir -p /etc/apt/keyrings

  53 curl -fsSL https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

  54 echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

  55 sudo apt-get update -y

  56 apt-cache madison kubeadm | tac

  57 sudo apt-mark unhold kubeadm && sudo apt-get update && sudo apt-get install -y kubeadm='1.32.3-1.1' && sudo apt-mark hold kubeadm

  58 history

before kill command execute drain command

        kubectl drain <nameofnode> --ignore-daemonsets --force
        killall -s SIGTERM kube-apiserver && sleep 20

        now upgrade:
          sudo kubeadm upgrade plan
          drain
          kubeadm upgrade apply v1.32.3

backup:
        try to execute
        delete pod
        change yaml file and bring it back
         2379,2380 -port of etcd
         6443 -api server (execute init and 3 commands otherwise listens to 8080 local host)-config file missing in ./kube

 


encrypt data in etcd (key)
                https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/

member list: gives the number of etcd running
	error: server messed up

consider 3 files while backup:
        configuration
        etcd
        certificate

/etc/kubernetes/manifests -static pod
                          move etcd.yaml -> no more static pod
                          if we bring back the etcd file -> become static pod
                          thats y worked

secure the data:
	create the right rbac(not enough)-still data in etcd as plain text
	encrypt data(keys)

get latest version 32 from 30
        kuberenetes kubeadm install documentation
                        key 
                        echo
                        update 
                        execute madison


JSON PATH AND JQ
        cpu utilization pod
        list the pod based on age


kublet goes down :
        config file wrongly configured

https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/jsonPath/Notes.txt

jq- 3rd party tool helps to querry anythg

machine issue,no pod should come -> cordon
dlete all pods -> drain

security in general:


cluster management:
        1. set up network
        2. secure controll plane
        3.secure worker node
        4.resource planning
        5. automation of cluster provisioning


if etcd file not therer iside /etc/kubernetes/manifests
        kubectl get pod -o yaml > file

        server certificte -> some command

crio client -> crictl

---------------------------------------------------------------------------------------
22/3/25

database-create stateful set
create a mysql pod and get inside (by creating deployment)
create service(clusterip) = kubectl expose deploy mysql-test --name=mysql-service --port=3306 --protocol=TCP --type=ClusterIP
building image
https://github.com/benstitou/kubernetes-spring-mysql-demo/tree/main/src/main/java/com/example/kubernetes
dif b/w mvn clean install and mvn package install
deployment:

apiVersion: apps/v1

kind: Deployment

metadata:

 labels:

  app: mysql-test

 name: mysql-test

spec:

 replicas: 1

 selector:

  matchLabels:

   app: mysql-test

 template:

  metadata:

   labels:

    app: mysql-test

  spec:

   containers:

   - image: mysql:8.0

    name: MySQL

    env:

     - name: MYSQL_ROOT_PASSWORD

      value: admin

     - name: MYSQL_DATABASE

      value: EventManagement

 ports:

  - containerPort: 3306

	

----------------------------------------------
end points:
-------------------------------------------------
helm:
--------------------
encrypt data in etcd: encryption configuration in api server file
-------------------------------------------------------------------
26/3/25
automatic bin packing:
        Automatic bin packing in Kubernetes refers to the process of 
                efficiently scheduling Pods on available Nodes 
                        based on their resource requests, limits, and constraints.
master node - control plane
worker node - data plane
network plugin is on every node
kubelet intimates api server about everything
admission controller


------------------------------------------
services:
        cluster ip -(supportts load balancing and service discovery)
                     2 pods within kubernetes cluster
service discovery:
         a new instance comes up/existing instances goes down

how does cluster ip know when new pod comes up?


        how cluster ip works?



        nordport:make services accessible to the trusted service within the cluster(my own babies)
        load balancer
        external ip


networking:
to address : service ip to pods ip
then forwarded back to bridge
bridge get msg : targeted to a pod(using pods ip)-deliver
bridge will take of delivering(same node or diff)
end points objects
dns -use coredns port to find ip of a service
     service ip


init containers:
        seperately run




CONFIGMAP AND SECRET NOTES:

Sure! Let me simplify it even more and **add comments** to the YAML code for clarity.  

---

## **💡 What is ConfigMap & Secret?**  
Think of **ConfigMap** as a **settings file** (like an `.ini` or `.env` file) that stores **non-sensitive** information, such as:  
- Database hostnames  
- Port numbers  
- Log levels  

A **Secret** is like a **password manager** that stores **sensitive** information, such as:  
- Passwords  
- API keys  
- Certificates  

---

## **🟢 1. How to Use ConfigMap?**  
We can inject **ConfigMap** into a pod in **two ways:**  
1. **As Environment Variables** (Static values)  
2. **As a Volume** (Reflects changes automatically)  

### **📌 1️⃣ Using ConfigMap as Environment Variables**
This method is like using `export VAR=value` in Linux.

#### **🔹 Step 1: Create a ConfigMap**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config  # Name of the ConfigMap
data:
  DB_HOST: "mysql.database"  # Storing database hostname
  DB_PORT: "3306"  # Storing database port number
```

#### **🔹 Step 2: Inject ConfigMap into a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      env:
        - name: DB_HOST  # Environment variable inside the pod
          valueFrom:
            configMapKeyRef:
              name: my-config  # Referring to ConfigMap we created above
              key: DB_HOST  # Fetching DB_HOST value from ConfigMap
```

✅ **What Happens?**  
- Inside the pod, you can access the value using:  
  ```bash
  echo $DB_HOST  # Output: mysql.database
  ```
❌ **Issue:** If the ConfigMap changes, the pod **won't** get updated unless restarted.

---

### **📌 2️⃣ Using ConfigMap as a Volume**
This method mounts the ConfigMap as **a file inside the pod**, so any changes in the ConfigMap **automatically** reflect in the pod.

#### **🔹 Step 1: Create a ConfigMap**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config  # Name of the ConfigMap
data:
  db-config: |
    DB_HOST=mysql.database
    DB_PORT=3306
```

#### **🔹 Step 2: Mount ConfigMap as a Volume in a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      volumeMounts:
        - name: config-volume
          mountPath: "/etc/config"  # Where ConfigMap will be mounted inside pod
  volumes:
    - name: config-volume
      configMap:
        name: my-config  # Referencing the ConfigMap
```

✅ **What Happens?**  
- Inside the pod, a file `/etc/config/db-config` is created with:  
  ```bash
  cat /etc/config/db-config
  # Output:
  # DB_HOST=mysql.database
  # DB_PORT=3306
  ```
- **If the ConfigMap is updated, the pod gets the changes immediately.**

---

## **🔴 2. How to Use Secrets?**
Just like ConfigMap, we can inject **Secrets** using **environment variables** or **volumes**.

### **📌 1️⃣ Using Secrets as Environment Variables**
#### **🔹 Step 1: Create a Secret**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: my-secret  # Name of the Secret
data:
  DB_PASSWORD: bXlzZWNyZXQ=  # "mysecret" encoded in Base64
```
> 🔹 To encode a value manually, use:
> ```bash
> echo -n "mysecret" | base64
> ```

#### **🔹 Step 2: Inject Secret into a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      env:
        - name: DB_PASSWORD  # Environment variable inside the pod
          valueFrom:
            secretKeyRef:
              name: my-secret  # Referring to the Secret
              key: DB_PASSWORD  # Fetching value from Secret
```

✅ **What Happens?**  
- Inside the pod, you can access the value using:  
  ```bash
  echo $DB_PASSWORD  # Output: mysecret
  ```
❌ **Issue:** If the Secret changes, the pod **won't** get updated unless restarted.

---

### **📌 2️⃣ Using Secrets as a Volume**
This method mounts the Secret as **a file inside the pod**, so any changes in the Secret **automatically** reflect in the pod.

#### **🔹 Step 1: Mount Secret as a Volume in a Pod**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod  # Name of the pod
spec:
  containers:
    - name: my-container  # Name of the container inside the pod
      image: nginx  # Example container image
      volumeMounts:
        - name: secret-volume
          mountPath: "/etc/secret"  # Where Secret will be mounted inside pod
  volumes:
    - name: secret-volume
      secret:
        secretName: my-secret  # Referencing the Secret
```

✅ **What Happens?**  
- Inside the pod, a file `/etc/secret/DB_PASSWORD` is created with the secret value:  
  ```bash
  cat /etc/secret/DB_PASSWORD
  # Output:
  # mysecret
  ```
- **If the Secret is updated, the pod gets the changes immediately.**

---

## **🛡️ 3. How to Encrypt Secrets in etcd?**
By default, **Secrets are only Base64-encoded, not encrypted**. We can encrypt them **before storing in etcd**.

#### **🔹 Encryption Configuration**
```yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources: ["secrets"]
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: c3VwZXJzZWNyZXQK  # Base64-encoded encryption key
      - identity: {}
```
After applying this, **Kubernetes will encrypt Secrets** before storing them in etcd.

---

## **🎯 Summary**
| Feature | ConfigMap (📜) | Secret (🔐) |
|---------|--------------|-------------|
| Purpose | Store settings (non-sensitive) | Store passwords, API keys (sensitive) |
| Example Data | `DB_HOST=mysql.server.com` | `DB_PASSWORD=MySecret123` |
| Security | Plain text | **Base64-encoded** (not fully encrypted) |
| Access Control | Less restricted | **More restricted (RBAC)** |
| Encryption | Not encrypted by default | Can be encrypted in etcd |

✔ **ConfigMap** = Non-sensitive settings (like DB host)  
✔ **Secret** = Sensitive data (like passwords)  
✔ **Use Environment Variables** for **static** values  
✔ **Use Volumes** for **dynamic** updates  
✔ **Encrypt Secrets in etcd** for better security  

---

### **🤔 Still Confused?**
Let me know **which part** you didn't understand, and I'll **simplify it even more**! 😊




sir code:


pod-with-config-map.
        apiVersion: v1  # API version used for creating a Kubernetes Pod.
        kind: Pod       # Specifies that this is a Pod definition.
        metadata:
        name: example-pod  # Name of the pod.

        spec:
        containers:
        - name: example-container  # Name of the container inside the pod.
            image: nginx  # Using the official Nginx image.

            env:  # Setting environment variables inside the container.
            - name: EXAMPLE_ENV_VAR  # Name of the environment variable.
            valueFrom:  # Getting value from an external ConfigMap.
                configMapKeyRef:
                name: example-configmap  # Name of the ConfigMap to reference.
                key: example-key  # The specific key inside the ConfigMap.


What This Does?
Creates a Pod named example-pod.
Runs an Nginx container inside that Pod.
Injects a value from ConfigMap into an environment variable inside the container.
The ConfigMap (example-configmap) has a key example-key with a value (example-value).
This value is assigned to an environment variable (EXAMPLE_ENV_VAR) inside the container.
The container can access this value as an environment variable.




RBAC NOTES:



## **🛠 Step 1: Create a New Namespace**
First, create a namespace where we will test the RBAC setup.  
```bash
kubectl create namespace test
```

---

## **👤 Step 2: Create a User (Without Admin Access)**
For this example, we will create a Kubernetes user **dicktracy**.  

### 🔹 Generate Certificates for User Authentication
Kubernetes uses **TLS certificates** for user authentication. We will create a certificate for `dicktracy`.  

```bash
# Generate a private key
openssl genrsa -out dicktracy.key 2048

# Generate a Certificate Signing Request (CSR)
openssl req -new -key dicktracy.key -out dicktracy.csr -subj "/CN=dicktracy/O=developers"

# Sign the CSR with Kubernetes CA to generate a client certificate
openssl x509 -req -in dicktracy.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out dicktracy.crt -days 365
```
👉 Now, `dicktracy` has a **certificate** but no permissions yet.

---

## **🔗 Step 3: Create a Role to List Pods**
Now, let's create a **Role** that allows a user to **only list pods** inside the `test` namespace.  

```yaml
# Save this file as pod-reader-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: test
  name: pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
```
🚀 **Apply the Role:**
```bash
kubectl apply -f pod-reader-role.yaml
```

---

## **🛠 Step 4: Bind the Role to `dicktracy`**
Now, create a **RoleBinding** to assign the `pod-reader` role to `dicktracy`.

```yaml
# Save this file as pod-reader-rolebinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: test
  name: user-pod-reader
subjects:
  - kind: User
    name: dicktracy  # Assigning permission to this user
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```
🚀 **Apply the RoleBinding:**
```bash
kubectl apply -f pod-reader-rolebinding.yaml
```

---

## **👀 Step 5: Verify Permissions for `dicktracy`**
Let's check if `dicktracy` can list pods in the `test` namespace.

```bash
kubectl auth can-i list pods --as=dicktracy -n test
```
👉 If the setup is correct, it should return:
```
yes
```

🚀 **Try listing pods (Should Work)**
```bash
kubectl get pods --as=dicktracy -n test
```

🚨 **Try deleting a pod (Should Fail)**
```bash
kubectl delete pod mypod --as=dicktracy -n test
```
👉 You should get a **permission denied** error.

---

## **🌎 Step 6: Assign Global Permissions (ClusterRole)**
Let's say we want **dicktracy** to list pods in **all namespaces**.

```yaml
# Save this file as cluster-pod-reader.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cluster-pod-reader
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
```
🚀 **Apply the ClusterRole:**
```bash
kubectl apply -f cluster-pod-reader.yaml
```

Now, create a **ClusterRoleBinding** to assign this ClusterRole to `dicktracy`.

```yaml
# Save this file as cluster-pod-reader-binding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: cluster-pod-reader-binding
subjects:
  - kind: User
    name: dicktracy
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-pod-reader
  apiGroup: rbac.authorization.k8s.io
```
🚀 **Apply the ClusterRoleBinding:**
```bash
kubectl apply -f cluster-pod-reader-binding.yaml
```

✅ **Test the new permissions**
```bash
kubectl get pods --as=dicktracy --all-namespaces
```
👉 Now `dicktracy` can **list pods in all namespaces**!

---

## **🎯 Summary**
| Command | Description |
|---------|------------|
| `kubectl create namespace test` | Create a namespace for testing. |
| `openssl genrsa -out dicktracy.key 2048` | Generate a private key for `dicktracy`. |
| `kubectl apply -f pod-reader-role.yaml` | Create a Role to allow listing pods. |
| `kubectl apply -f pod-reader-rolebinding.yaml` | Bind the Role to `dicktracy`. |
| `kubectl auth can-i list pods --as=dicktracy -n test` | Check if `dicktracy` has permission. |
| `kubectl apply -f cluster-pod-reader.yaml` | Create a ClusterRole to list pods in all namespaces. |
| `kubectl apply -f cluster-pod-reader-binding.yaml` | Bind the ClusterRole to `dicktracy`. |

---

## **🚀 Next Steps**
Would you like to test this in a **real Kubernetes cluster**? Or do you need help setting up a Kubernetes cluster for practice? 😊



pod working:
creation of pod:
/.kube/config-context -certificate- signed by kubernetes client->kubectl(restapi req)->info go to header
master -control plane
worker-data plane 
certfile,keyfile - authentication
roles,rolebindings,clusterrole and binding - autherization(data in etcd)
api - event driven architecture
scheduler - finds best node to ocreate a pod,scheduler will get triggered(etcd -how much info there in cpu(worker))
controller - create a pod,manages the pod   
                replica,node,pod,deployment,job controller -> packaged into single


worker:
proxy : knows everythg about networking
        ip
        pods associated with the ip
etcd : no sql ,data comes to etcd
        statefull - persist -scale horizontally

scheudler : picks the node
controller comes into picture when actual state not equal to current
then goes to api server and to kubelet work with container d creates pod..come back to api server then to etcd -> running
kubelet knows how much menory is there..updates the etcd.



crud operation happens b/w api server and etcd
scheduler - 

creating a pod wht happens:
        pod table:
        slno     name      image    curret-state     desired state   node
        1       ustpod       ngninx                     running         

        kubectl to api server-create pod
---------------------------------------------------------------
replica set created:
        rs table :
                sno 
                name 
                desired replicas
                actual replicas
                image name
kubectl looks current context ( get certificate and key)
convert rest api req and sents -> api server after 4 sets makes an entry
checks desired and actual -> replication controller comes -> create pods
tABLE:                                                                                          
        sno 
        name 
        image
        desired
        actual

        schedueler -> picks the node
        controller -> creates the pod
        cri

if new entry came,scheduler will come
node goes down -> kubelett api sever communicate
replicaset-
        self healing
        scale in,out


-----------------------------------------------------------------

DEPLOYMENT:
        Sl no 
        name 
        current state
        desired
        image 

        deployment controller comes into picture -> create replica set
        replication controller watches and make entry in pod table
        scheduler watches -> update the machine where created
        controller - 
        api server talks to kubet and created

rolling update(n num of pods will come up once active) and roll back 
        other staretegies support by kubernetics apart from rolling update->
                                  recreate(deployment strategy)- remove all pods and create all set of pods
update :
        new entry in replicaset table
        increase count of that and reduce count of old one in replica set and new pod of this verrsion created and the countb of replica set oledr entry reduces
        look ppt  
        kubectl set image deploy deployment name nginx=nginx:1.16.1

------------------------------------------------------------
how cluster ip works,Networking:
        2 containers in same pod talk.
                create namespace and attched to same name space
                talk using local host
                done by kubernetes(kubectl)
        2 continers of 2 diff pods:
                custom bridge created and all attached to that(ip a = we can see)
                custom bridge=act like a switch,forward the req
                network switch:
                        modem @ home 
                        when i connect 1 device to 1 ethernet port and other on other
                        arp protocol
                                sent the broadcast ip(last ip),handshake singnal to all devices
                                reply back with details
                                so switch gets ip address and mac 
                                record this ip belongs to this mac
                                this is how switch works
                                if the ip address is there sent to bridge,otherwise default root
                                default root go by gateway
                                default root connecting to network plugin
                                2 diff pods = ip not in range ,go to nrtwork plugin,another node netwrk plugin
        service to pod:
                creating cluster ip
                ip routing table
                when ever the entry in ip table ,cbr forward req to kubeproxy
                proxy = having list of service ip,pod's ip
                end points=when describe the service,kubectl sent info about pods to api
                kubeproxy sync info to api server
                do loadbalancing n=b/w them
                nating hapening(address got changed)
                msg sent back to bridge
                bridge sent to right machine



how networking works:
        req will be only forwarded to kube proxy when the cluster ip is on the cidr range
        nodeport will forward to cluster ip
        nodeport
                (know ip address and accesible)
        loadbalancer 
                2 levels - find service
                          forward to one of the instance
                each of the service create diff load balancer(login,)
                cannot give this to customer 
                solution: 1 public ip
                          login - go to login
                kubernetes service - do load balancing -forward to 1 of the pod 
        local dns is configured to talk with google dns
--------------------------------------------------------------
create service 
        so create deployment instesd of pod
service customizer cidr change manually:
                  change  proxy
                  change api server
                
-------------------------------------------------------------------
DNs:
get inside pod 
        when u hit cluster ip -finds the ip
        see the name server =10.96.0.10-kube-dns(resolve.conf)
        forwarding req to pod which has labelled
        selector in pod 
        give as label

Ingress:
        baremetal ingress controller installed in ur envt
        13th line ingress simple instruction.txt 
        httpd - shows -it works
        forwarding req to nodeport associated with ingress controller
        ingress controller referring to ingress rules
        these are 2 levels of load balancing

scheduling:
        




clusterip:

 git clone https://github.com/vilasvarghese/docker-k8s
  157  cd docker-k8s/yaml/services/
  158  ls
  159  cat cluster-ip.yaml 
  160  kubectl get pod,svc
  161  kubectl delete pods --all
  162  kubectl get pod,svc
  163  kubectl apply -f cluster-ip.yaml 
  164  kubectl get svc
  165  kubectl get pods
  166  curl 10.107.138.218
  167  kubectl run nginx --image=nginx
  168  curl 10.107.138.218
  169  kubectl get pods -o wide
  170  curl 10.107.138.218
  171  kubectl describe svc my-cluster-ip 
  172  kubectl run -l mginx --image=nginx
  173  kubectl get pods --show-labels
  174  curl 10.107.138.21
  175  curl 10.107.138.218:80
  176  curl 10.107.138.218
  177  kubectl get svc
  178  curl 10.107.138.218
  179  curl 10.107.138.218:80
  180  kubectl get pods
  181  kubectl get po -o wide
  182  curl 10.32.0.6
  183  kubectl get svc
  184  curl 10.107.138.218
  185  kubectl describe svc my-cluster-ip
  186  kubectl get po -l app=my-cluster-ip
  187  kubectl get po --show-labels
  188  kubectl edit svc my-cluster-ip
  189  kubectl get po -l app=nginx
  190  kubectl edit svc my-cluster-ip
  191  kubectl describe svc my-cluster-ip
  192  kubeclt get svc
  193  kubectl get svc
  194  curl 10.107.138.218
  --------------------------------------------
  lb -create ext load balancer(aws) - public ip dns
  use nort port - 

ingress- path-based-routing(alowing)
           
network policy : restriction 
------------------------------------------------------
api server-etcd -husband wife relation



COMMANDS:

day1

 kubectl run newpod --image=nginx

 101 kubectl gey pod

 102 kubectl get pod

 103 kubectl describe newpod

 104 kubectl describe pod newpod

 105 kubectl get pods -n kube-system

 106 curl 172.16.77.133

 107 cd docker-k8s/

 108 cd ..

 109 kubectl patch serviceaccount default -p '{"imagePullSecrets": [{"name": "regcred"}]}'

 110 cd docker-k8s/

 111 cd yaml/deployment/

 112 ls

 113 cat ng-pod.yaml

 114 kubectl apply -f ng-pod.yaml

 115 kubectl get pod

 116 kubectl get pod newpod yaml

 117 kubectl get pod newpod -o yaml

 118 kubectl get pod newpod -o yaml > newpod.yaml

 119 ls

 120 nano newpod.yaml

 121 kubectl run newpod2 --image=nginx --dry-run=client -o yaml > newpod1.yaml

 122 ls

 123 nano newpod1.yaml

 124 cat replica-set.yaml

 125 kubectl api-resources |grep -i replicaset

 126 nano replica-set.yaml

 127 kubectl apply -f replica-set.yaml

 128 kubectl delete pod --all

 129 nano replica-set.yaml

 130 kubectl get secrets

 131 kubectl get ns

 132 nano replica-set.yaml

 133 kubectl get secrets -n kube-system

 134 kubectl get secrets

 135 kubectl get secrets -n default

 139 kubectl get pod

 140 kubectl get pod -o kube-system

 141 kubectl get pod -n kube-system

 142 nano replica-set.yaml

 143 kubectl apply -f replica-set.yaml

 144 kubectl edit rs frontend

 145 kubectl scale rs frontend --replicas=3

 146 nano replica-set.yaml

 147 kubectl get pod -o wide

 148 kubectl scale rs frontend --replicas=5

 149 kubectl get pod -o wide

 150 kubectl delete pod --all

 151 kubectl get pod -o wide

 152 kubectl delete rs --all

 153 ls

 154 cat deploy-ng.yaml

 155 kubectl apply -f deploy-ng.yaml

 156 kubectl get pod

 157 kubectl get deploy rs,pod

 158 kubectl get deploy

 159 kubectl describe deploy nginx-deployement

 160 kubectl describe deploy nginx-deployment

 161 kubectl rollout status deploy nginx-deployment

 162 kubectl scale deploy nginx-deployment --replicas=5

 163 kubectl set image deploy nginx-deployment nginx=nginx:1.16.1

 164 kubectl describe pod nginx-deployment-8d94c585f-4rmkz

 165 kubectl rollout history deploy nginx-deployment

 166 kubectl annotate deploy nginx-deployment kubernetes.io/change-cause="upgraded from failing version 1.9.1 to 1.16.1"

 167 kubectl rollout history deploy nginx-deployment



---label and name spaces-----



 kubectl apply -f label-selector.yaml

 22 kubectl get pod

 23 kubectl get pod -l env=development

 24 kubectl get ns

 25 kubectl create ns vnyns

 26 kubectl run pod -n vnyns --image=nginx

 27 kubectl get po

 28 kubectl get po -n vnyns

------rolling update-------

 kubectl scale deploy nginx-deployment --replicas=5

 47 kubectl set image deploy nginx-deployment nginx=nginx:1.16.1

 48 kubectl describe pod nginx-deployment-8d94c585f-c9jdq

 49 kubectl set image deploy nginx-deployment nginx=nginx:1.16.1

 50 kubectl set image deploy nginx-deployment nginx=nginx:1.18.1

 51 kubectl set image deploy nginx-deployment nginx=nginx:1.20.1

 52 kubectl rollout history deploy nginx-deployment

 53 kubectl annotate deploy nginx-deployment kubernetes.io/change-cause="upgra ded from failing version 1.18.1 to 1.20.1"

 54 kubectl rollout history deploy nginx-deployment





https://github.com/vilasvarghese/docker-k8s/blob/master/yaml/ingress/simple/instructions.txt



--------------------------------------------------------------------------------
hostnamectl set-hostname kmaster
    2  hostname
    3  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
    4  overlay
    5  br_netfilter
    6  EOF
    7  sudo modprobe overlay
    8  sudo modprobe br_netfilter
    9  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
   10  net.bridge.bridge-nf-call-iptables = 1
   11  net.bridge.bridge-nf-call-ip6tables = 1
   12  net.ipv4.ip_forward = 1
   13  EOF
   14  sudo sysctl --system
   15  sudo swapoff -a
   16  (crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true
   17  sudo apt-get update -y
   18  sudo apt-get install -y software-properties-common gpg curl apt-transport-https ca-certificates
   19  curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
   20  echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /" | tee /etc/apt/sources.list.d/cri-o.list
   21  sudo apt-get update -y
   22  sudo apt-get install -y cri-o
   23  sudo systemctl daemon-reload
   24  sudo systemctl enable crio --now
   25  sudo systemctl start crio.service
   26  systemctl status crio
   27  systemctl start crio
   28  VERSION="v1.32.0"
   29  wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
   30  sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
   31  rm -f crictl-$VERSION-linux-amd64.tar.gz
   32  KUBERNETES_VERSION=1.32
   33  curl -fsSL https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   34  echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
   35  apt update -y
   36  sudo apt-get install -y kubelet kubeadm kubectl
   37  kubedm version
   38  kubeadm version
   39  kubelet version
   40  kubectl version
   41  kubeadm init
   42  kubeadm reset
   43  kubeamd init
   44  kubeadm init
   45  kubectl get nodes
   46  kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
   47  watch -n 1 kubectl get po -o wide
   48  watch -n 1 kubectl get pds -n kube-system
   49  watch -n 1 kubectl get pods -n kube-system
   50  hostnamectl set-hostname kmaster
   51  hostname
   52  kubectl get nodes
   53  watch -n 1 kubectl get pods -n kube-system
   54  kubectl get nodes
   55  watch -n 1 kubectl get pods -n kbe-system
   56  watch -n 1 kubectl get pods -n kube-system
   57  hostname
   58  cd 
   59  kubectl get nodes
   60  kubectl top node
   61  kubectl top pod
   62  git clone https://github.com/vilasvarghese/docker-k8s
   63  cd docker-k8s/yaml/metricServer/
   64  kubectl apply -f metric-server.yaml 
   65  vi metric-server.yaml 
   66  kubectl top node
   67  kubectl top node -n kube-system
   68  kubectl top pod
   69  kubectl top pod -n kube-system
   70  vi deployment.yaml
   71  kubectl apply -f deployment.yml
   72  kubectl apply -f deployment.yaml
   73  kubectl get deploy
   74  vi service.yaml
   75  kubectl apply -f service.yaml
   76  kubectl get svc
   77  vi hpa.yaml
   78  kubectl apply -f hpa.yaml
   79  watch -n 1 kubectl get hpa
   80  kubectl get deploy,svc,hpa
   81  kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://hpa-demo-deployment; done"
   82  kubectl get hpa
   83  kubectl top pod
   84  watch -n 1 kubectl get pod
   85  watch -n 1 kubectl top pod
   86  alias k=kubectll
   87  k get pod -o wide
   88  alias k=kubectl
   89  k get pod -o wide
   90  k get node
   91  cd docker-k8s/yaml/scheduling/
   92  ls
   93  vi ScheduleWithNodeName.yml 
   94  kubectl apply -f ScheduleWithNodeName.yml 
   95  k get pod -o wide
   96  k get po -o wide
   97  ls
   98  kubectl apply -f ScheduleWithNodeName.yml 
   99  kubectl get po -o wide
  100  kubectl describe po nginx
  101  vi ScheduleWithNodeName.yml 
  102  kubectl get node
  103  hostname
  104  vi ScheduleWithNodeName.yml 
  105  kubectl apply -f ScheduleWithNodeName.yml 
  106  kubectl get po -o wide
  107  vi ScheduleWithNodeName.yml 
  108  kubectl apply -f ScheduleWithNodeName.yml 
  109  kubect delete pod nginx
  110  kubectl apply -f ScheduleWithNodeName.yml 
  111  vi ScheduleWithNodeName.yml 
  112  kubectl apply -f ScheduleWithNodeName.yml 
  113  k delete pod nginx
  114  kubectl apply -f ScheduleWithNodeName.yml 
  115  kubectl get po -o wide
  116  ls
  117  vi NodeSelector.yml 
  118  k apply -f NodeSelector.yml 
  119  k get pod -o wide
  120  vi NodeSelector.yml 
  121  k label nodes ip-172-31-39-119 employee=abhirami
  122  k apply -f NodeSelector.yml 
  123  kubectl delete pod cuda-test
  124  vi NodeSelector.yml 
  125  k apply -f NodeSelector.yml 
  126  k get pod -o wide
  127  k delete po --all
  128  k get pod -o wide
  129  sudo su
  130  kubectl get nodes
  131  kubectl taint nodes ip-172-31-39-119 app=DBNode:NoSchedule
  132  k describe ip-172-31-39-119
  133  kubectl describe node ip-172-31-39-119
  134  cd docker-k8s/yaml/scheduling/
  135  kubectl describe node ip-172-31-39-119
  136  ls
  137  kubectl apply -f taintsAndTolerations/PodWithTolerations.yaml 
  138  kubectl get po -o wide
  139  history
  140  kubectl get nodes
  141  kubectl secribe node ip-172-31-36-90 | grep Taint
  142  kubectl decribe node ip-172-31-36-90 | grep Taint
  143  kubectl describe node ip-172-31-36-90 | grep Taint
  144  kubectl describe node ip-172-31-36-90 | grep -i Taint
  145  kubectl describe node ip-172-31-36-90 | grep -i taint
  146  kubectle get nodes
  147  kubectl get nodes
  148  kubectl describe node ip-172-31-46-172 | grep -i taint
  149  LS
  150  ls
  151  cd taintsAndTolerations/
  152  ls
  153  vi PodWithTolerations.yaml 
  154  kubectl apply -f PodWithTolerations.yaml 
  155  kubectl delete pod nginx
  156  kubectl apply -f PodWithTolerations.yaml
  157  kubectl get pods -o wide
  158  ~kubectl get pods -o wide
  159  kubectl get pods -o wide
  160  cd ..
  161  kubectl taint nodes ip-172-31-39-119 app=DBNode:NoSchedule
  162  kubectl taint nodes ip-172-31-39-119 app=DBNode:NoSchedule-
  163  kubectl get pod -o wide
  164  history
  165  kubectl get pod -o wide
  166  kubectl get nodes
  167  kubectl describe node ip-172-31-36-90 | grep Taint
  168  kubectl taint nodes ip-172-31-36-90 app=DBNode:NoSchedule
  169  kubectl describe node ip-172-31-36-90 | grep Taint
  170  kubectl delete pod nginx
  171  kubectl apply -f taintsAndTolerations/PodWithTolerations.yaml 
  172  kubectl get pods
  173  kubectl get pods -o wide
  174  cat taintsAndTolerations/PodWithTolerations.yaml 
  175  kubectl get nodes
  176  kubectl describe node ip-172-31-39-119 | grep -i taint
  177  kubectl taint nodes ip-172-31-39-119 app=DBNode:NoSchedule
  178  kubectl describe node ip-172-31-39-119 | grep -i taint
  179  kubectl delete pod nginx
  180  kubectl apply -f taintsAndTolerations/PodWithTolerations.yaml 
  181  kubectl get pods
  182  kubectl get pods -o wide
  183  kubectl describe node ip-172-31-39-119 | grep -i taint
  184  cat taintsAndTolerations/PodWithTolerations.yaml 
  185  kubectl get pods -o wide
  186  cd ..
  187  cd jobs
  188  ls
  189  cd job/
  190  ls
  191  cat job.yaml 
  192  kubectl logs countdown-9tmxf
  193  kubectl get pod -o wide
  194* 
  195  kubectl get pod -o wide
  196  kubectl logs countdown-sdds6
  197  kubectl get pod -o wide
  198  kubectl get job
  199  ls
  200  kubectl get pod -o wide
  201  kubectl describe pod countdown-sdds6
  202  kubectl taint nodes kworker1 app=DBNode:NoSchedule-
  203  kubectl get nodes
  204  kubectl taint nodes ip-172-31-46-172 app=DBNode:NoSchedule-
  205  kubectl taint nodes ip-172-31-39-119 app=DBNode:NoSchedule-
  206  kubectl taint nodes ip-172-31-36-90 app=DBNode:NoSchedule-
  207  kubectl get nodes
  208  kubectl get pod -o wide
  209  kubectl delete pod countdown-sdds6
  210  kubectl apply -f job.yaml 
  211  kubectl get pod -o wide
  212  kubectl create -f job.yaml 
  213  kubectl delete jobs --all
  214  kubectl create -f job.yaml 
  215  kubectl get pod -o wide
  216  kubectl get jobs
  217  ls
  218  cat cronejob.yaml
  219  cat cronjob.yaml 
  220  kubectl apply -f cronjob.yaml 
  221  kubectl get pods -o wide
  222  watch -n 1 get pods -o wide
  223  watch -n 1 get pods
  224  history
  ---------------------------------------------------------------------

day 3
RBAC

git clone https://github.com/vilasvarghese/docker-k8s

  47 ls

  48 export MAGIC_USER=dicktracy

  49 echo $MAGIC_USER

  50 openssl genrsa -out dicktracy.key 2048

  51 ls

  52 cat dicktracy.key

  53 openssl req -new -key dicktracy.key -out dicktracy.csr -subj "/CN=${MAGIC_USER}/O=devs/O=tech-leads

  54 openssl req -new -key dicktracy.key -out dicktracy.csr -subj "/CN=${MAGIC_USER}/O=devs/O=tech-leads"

  55 ls

  56 cat dicktracy.csr

  57 sudo openssl x509 -req -in dicktracy.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out dicktracy.crt -days 500

  58 ls

  59 cat dicktracy.crt

  60 mkdir -p $HOME/.certs && mv dicktracy.crt dicktracy.key $HOME/.certs

  61 cd $HOME

  62 cd ~/$HOME/.cert

  63 cd ~/$HOME/.certs

  64 ls

  65 cd ..

  66 ls

  67 cd ~/$HOME/.certs

  68 ls -a

  69 cd ~

  70 ls

  71 cd docker-k8s/yaml/rbac/

  72 ls

  73 cat namespaces.yaml

  74 kubectl apply -f namespaces.yaml

  75 kubectl get pod,ns

  76 kubectl config view

  77 kubectl config get-contexts

  78 kubectl config set-credentials ${MAGIC_USER}@kubernetes --client-certificate=$HOME/.certs/${MAGIC_USER}.crt --client-key=$HOME/.certs/${MAGIC_USER}.key --embed-certs=true

  79 kubectl config view

  80 kubectl config get-contexts

  81 kubectl config set-context ${MAGIC_USER}@kubernetes --cluster=kubernetes --user=${MAGIC_USER}@kubernetes

  82 kubectl config get-contexts

  83 ls

  84 vi pod-reader.yaml

  85 kubectl apply -f pod-reader.yaml

  86 kubectl get role -n test

  87 cat user-pod-reader-rolebinding.yaml

  88 kubectl apply -f user-pod-reader-rolebinding.yaml

  89 nano user-pod-reader-rolebinding.yaml

  90 kubectl apply -f user-pod-reader-rolebinding.yaml

  91 kubectl kubectl get rolebinding -n test

  92 kubectl get rolebinding -n test

  93 kubectl run npod --image=nginx -n test

  94 kubectl get pod -n test

  95 kubectl describe pod npod

  96 kubectl describe node kmaster

  97 kubectl describe node kmaster |taint

  98 kubectl describe node kmaster |grep taint

  99 kubectl describe node kmaster | grep "taint"

 100 kubectl describe node kmaster | grep "Taint"

 101 kubectl taint nodes kmaster node-role.kubernetes.io/control-plane:NoSchedule-

 102 kubectl get pod -n test

 103 kubectl auth can-i list pods -n test --as dicktracy@kubernetes

 104 kubectl auth can-i list pods -n test --as kubernetes-admin@kubernet

 105 kubectl config use-context ${MAGIC_USER}@kubernet

 106 kubectl config get-context

 107 kubectl config get-contexts

 108 kubectl auth can-i list pods -n test --as dicktracy

 109 kubectl auth can-i list pods -n test --as dicktracy@kubernetes

 110 kubectl get pod -n test

 111 kubectl run newpod --image=nginx -n test

 112 kubect list pod -n test

 113 kubectl list pod -n test

 114 kubectl config use-context kubernetes-admin@kubernetes

 115 kubectl auth can-i list pods -n test --as dicktracy

 116 ls

 117 nano simple-dev-role.yaml

 118 kubectl apply -f simple-dev-role.yaml

 119 nano simple-dev-role.yaml

 120 kubectl apply -f simple-dev-role.yaml

 121 kubectl get role -n devs

 122 nano simple-dev-rolebinding.yaml

 123 kubectl apply -f simple-dev-rolebinding.yaml

 124 kubectl get rolebinding -n devs

 125 kubectl config use-contexts dicktracy@kubernetes

 126 kubectl config use-context dicktracy@kubernetes

 127 kubectl run newpod --image=nginx -n devs

 128 kubectl get pod -n devs

 129 vi simple-dev-role.yaml

 130 cd ..

 131 ls

 132 cd serviceAccount/

 133 ls

 134 kubectl create deployment nginx1 --image=nginx

 135 kubectl config use-context kubernetes-admin@kubernetes

 136 kubectl create deployment nginx1 --image=nginx

 137 kubectl get deploy

 138 kubectl get pod

 139 kubectl describe pod nginx1-b7c99b5c8-hm5tj

 140 kubectl exec -it nginx1-b7c99b5c8-hm5tj -- /bin/bash

 141 kubectl et serviceaccount

 142 kubectl get serviceaccount

 143 kubectl create rolebinding nginx-sa-readonly --clusterrole=view --serviceaccount=default:nginx-serviceaccount --namespace=default

 144 ls

 145 cat dep.yaml

 146 kubectl apply -f dep.yaml

 147 kubectl get pod

 148 hostnamectl set-hostname kmaster

 149 sudo su

 150 cd docker-k8s/yaml/serviceAccount/

 151 kubectl get deploy,pod

 152 kubectl exec -it pod/nginx-deployment-78f76d5775-k4mtd --/bin/bash

 153 kubectl exec -it pod/nginx-deployment-78f76d5775-k4mtd -- /bin/bash

 154 ls

 155 cd docker-k8s/yaml/

 156 ccd secrets

 157 cd secrets/

 158 ls

 159 kubectl create secret generic test --from

 160 echo "admin" > username.txt

 161 echo "password" > password.txt

 162 cat username.txt

 163 cat password.txt

 164 kubectl create secret generic test --from-file=./username.txt --from-file=./password.txt

 165 kubectl get secret

 166 kubectl describe secret test

 167 cat secrets.yaml

 168 echo -n 'admin'|base64

 169 echo bXlwYXNzd29yZA== |base64 --decode

 170 kubectl apply -f secrets.yaml

 171 kubectl get secrets

 172 echo -n 'password'|base64

 173 kubectl get secret test -o yaml > examplesecret.yaml

 174 cat examplesecret.yaml

 175 cat secrets-env-pod.yaml

 176 kubectl describe secrets.yaml

 177 kubectl describe secret mysecret

 178 kubectl apply -f secrets-env-pod.yaml

 179 kubectl get pod

 180 kubectl exec -it secret-env-pod -- /bin/bash

 181 cat secrets-use.yaml

 182 kubectl apply -f secrets-use.yaml

 183 kubectl get pod

 184 kubectl exec -it mysecret-pod -- /bin/bash

  1 cd /etc/foo

  2 ls

  3 cat user

  4 cat password

 186 cd ..

 187 cd configmap/simple/

 188 ls

 189 cat configmap.yml

 190 kubectl apply -f configmap.yml

 191 kubectl get cm

 192 kubectl describe cm example-configmap

 193 cat pod-with-config-map.yml

 194 kubectl get pod

 195 kubectl apply -f pod-with-config-map.yml

 196 kubectl get pod

 197 kubectl exec -it example-pod -- /bin/bash

 198 cd ..

 199 cd volumes/

 200 ls

 201 cd empty-dir.yaml

 202 cat empty-dir.yaml

 203 kubectl apply -f empty-dir.yaml

 204 kubectl get pod

 205 kubectl exec -it my-empty-dir -c test-container-1 -- /bin/ash

 206 cd docker-k8s/yaml/volumes/

 207 ls

 208 kubectl get pod

 209 kubectl exec -it my-empty-dir -c test-container-1 -- /bin/ash

 210 cat host-path.yaml

 211 kubectl get pod

 212 kubectl apply -f host-path.yaml

 213 kubectl get pod

 214 kubectl exec -it redis-hostpath -- /bin/bash

  1 cd /test-mnt/

  4 touch abc.txt

  5 ls

  6 history
  ---------------------------------------------------------
20/3/2025

  cd docker-k8s/yaml/volumes/
   52  ls
   53  cat persistent-volume.yaml 
   54  kubectl apply -f persistent-volume.yaml 
   55  kubectl get pv
   56  cat persistent-volume-claim.yaml 
   57  kubectl apply -f persistent-volume-claim.yaml 
   58  kubectl get pvc
   59  kubectl get pv
   60  ls
   61  cat persistent-volume-pod.yaml 
   62  kubectl apply -f persistent-volume-pod.yaml 
   63  kubectl get pv-pod
   64  kubectl get pod
   65  kubectl exec -it pv-pod -- /bin/ash
   66  cd docker-k8s/yaml/volumes/
   67  kubectl get pods
   68  kubectl exec -it pv-pod -- /bin/bash
   69  kubectl exec -it pv-pod -- /bin/ash
   70  clear
   71  kubectl delete pv --all
   72  kubectl get po
   73  kubectl get pvc
   74  kubectl get pv
   75  kubectl delete po --all
   76  kubectl get pv
   77  kubectl get pvc
   78  kubectl delete pvc --all
   79  kubectl get pvc
   80  kubectl get pv
   81  kubectl get nodes
   82  kubectl describe node kmaster | grep -i taint
   83  kubectl taint node kmaster node-role.kubernetes.io/control-plane:NoSchedule-
   84  kubectl get nodes
   85  ]
   86  nano liveliness.yaml
   87  kubectl apply -f liveliness.yaml 
   88  kubectl get pods
   89  kubectl describe pod liveness-exec
   90  nano readliness.yaml
   91  kubectl apply -f readliness.yaml 
   92  kubectl get pods
   93  kubectl describe pod goproxy
   94  nano readliness.yaml
   95  nano liveliness.yaml
   96  nano readliness.yaml
   97  kubectl apply -f readliness.yaml 
   98  kubectl get pods
   99  kubectl describe pod readylines-exec
  100  nano readliness.yaml
  101  kubectl apply -f readliness.yaml 
  102  kubectl get pods
  103  kubectl delete pod readylines-exec
  104  kubectl apply -f readliness.yaml 
  105  kubectl get pods
  106  kubectl describe pod readylines-exec
  107  kubectl get pods
  108  kubectl run beforebackup --image=nginx
  109  kubectl get pod
  110  etcdctl
  111  apt  install etcd-client  
  112  ETCDCTL_API=3 etcdctl            --endpoints=https://[127.0.0.1]:2379            --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key               member list
  113  cd etc
  114  cd /etc
  115  cd/kubernetes
  116  cd kubernetes/manifests/
  117  ls
  118  cat etcd.yaml
  119  ETCDCTL_API=3 etcdctl            --endpoints=https://127.0.0.1:2379              --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key snapshot save /tmp/snapshot.db 
  120  cd /tmp/
  121  ls
  122  cat snapshot.db 
  123  cd /etc
  124  cd kubernetes/manifests/
  125  ls
  126  history
  127  ETCDCTL_API=3 etcdctl            --endpoints=https://127.0.0.1:2379              --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key snapshot status /tmp/snapshot.db 
  128  cd
  129  kubectl run afterbackup --image=nginx
  130  kubectl get pod
  131  kubectl delete pod beforebackup
  132  ETCDCTL_API=3 etcdctl            --endpoints=https://127.0.0.1:2379              --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key --data-dir=/var/lib/etcd.dir snapshot restore /tmp/snapshot.db 
  133  kubectl get pod
  134  mv /var/lib/etcd /var/lib/etcd/etcd-backup
  135  mv /var/lib/etcd /var/lib/etcd-backup
  136  kubectl get po
  137  ls
  138  mv /var/lib/etcd.dir /var/lib/etcd
  139  kubectl get po -o wide
  140  kubectl get po -n kube-system
  141  kubectl delete pod etcd-kmaster -n kube-system
  142  cd 
  143  cd /home/ubuntu/
  144  cd /etc/kubernetes/manifests/
  145  ls
  146  cd 
  147  cd /home/ubuntu/
  148  cd /var/lib/
  149  ls
  150  etcdutl --data-dir /var/lib/etcd snapshot restore /tmp/snapshot.db
  151  hstory
  152  history

---------------------------
etcd backup - done

ETCDCTL_API=3 etcdctl            --endpoints=https://127.0.0.1:2379              --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key snapshot status /tmp/snapshot.db 
  128  cd
  129  kubectl run afterbackup --image=nginx
  130  kubectl get pod
  131  kubectl delete pod beforebackup
  132  ETCDCTL_API=3 etcdctl            --endpoints=https://127.0.0.1:2379              --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key --data-dir=/var/lib/etcd.dir snapshot restore /tmp/snapshot.db 
  133  kubectl get pod
  134  mv /var/lib/etcd /var/lib/etcd/etcd-backup
  135  mv /var/lib/etcd /var/lib/etcd-backup
  136  kubectl get po
  137  ls
  138  mv /var/lib/etcd.dir /var/lib/etcd
  139  kubectl get po -o wide
  140  kubectl get po -n kube-system
  141  kubectl delete pod etcd-kmaster -n kube-system
  142  cd 
  143  cd /home/ubuntu/
  144  cd /etc/kubernetes/manifests/
  145  ls
  146  cd 
  147  cd /home/ubuntu/
  148  cd /var/lib/
  149  ls
  150  etcdutl --data-dir /var/lib/etcd snapshot restore /tmp/snapshot.db
  151  hstory
  152  history
  153  cd
  154  cd /home/ubuntu/
  155  cd /var/lib/
  156  ls
  157  ETCDCTL_API=3 etcdctl            --endpoints=https://127.0.0.1:2379              --cacert=/etc/kubernetes/pki/etcd/ca.crt                --cert=/etc/kubernetes/pki/etcd/server.crt               --key=/etc/kubernetes/pki/etcd/server.key --data-dir=/var/lib/etcd.dir snapshot restore /tmp/snapshot.db 
  158  ls
  159  cd /etc/kubernetes/manifests/
  160  kubectl get po
  161  mv etcd.yaml /tmp/etcd.yaml
  162  mv /tmp/etcd.yaml .
  163  kubectl get po
  164  watch -n 1 get po 
  165  vi etcd.yaml 
  166  cd /var/lib/
  167  ls
  168  cd -
  169  vi etcd.yaml 
  170  watch -n 1 kubectl get po
  171  ls
  172  kubectl get po
  173  history

  ---------------------------------------------------------------------------------
  21/3/25

   clear
    2  cd 
    3  clear
    4  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
    5  overlay
    6  br_netfilter
    7  EOF
    8  sudo modprobe overlay
    9  sudo modprobe br_netfilter
   10  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
   11  net.bridge.bridge-nf-call-iptables  = 1
   12  net.bridge.bridge-nf-call-ip6tables = 1
   13  net.ipv4.ip_forward                 = 1
   14  EOF
   15  sudo sysctl --system
   16  sudo swapoff -a
   17  (crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true
   18  sudo apt-get update -y
   19  sudo apt-get install -y software-properties-common gpg curl apt-transport-https ca-certificates
   20  curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key |
   21  curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key |     gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
   22  echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /" |     tee /etc/apt/sources.list.d/cri-o.list
   23  sudo apt-get update -y
   24  sudo apt-get install -y cri-o
   25  sudo systemctl daemon-reload
   26  sudo systemctl enable crio --now
   27  sudo systemctl start crio.service
   28  wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
   29  sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
   30  rm -f crictl-$VERSION-linux-amd64.tar.gz
   31  KUBERNETES_VERSION=1.31
   32  sudo apt-get update -y
   33  apt-cache madison kubeadm | tac
   34  sudo apt-get install -y kubelet=1.31.7-1.1 kubectl=1.31.7-1.1 kubeadm=1.31.7-1.1
   35  sudo apt-mark hold kubelet kubeadm kubectl
   36  sudo apt-get install -y jq
   37  KUBELET_EXTRA_ARGS=--node-ip=$local_ip
   38  EOF
   39  hostname
   40  export KUBECONFIG=/etc/kubernetes/admin.conf
   41  kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
   42  kubectl get nodes
   43  kubectl describe node master | grep -i taint
   44  kubectl taint node master node-role.kubernetes.io/control-plane:NoSchedule-
   45  kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
   46  kubectl run mypod --image=nginx
   47  kubectl get pods
   48  sudo apt update
   49  sudo apt-cache madison kubeadm
   50  KUBERNETES_VERSION=1.32
   51  sudo mkdir -p /etc/apt/keyrings
   52  curl -fsSL https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   53  echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
   54  history
   55  sudo apt-get update -y
   56  sudo apt-mark unhold kubeadm && sudo apt-get update && sudo apt-get install -y kubeadm='1.32.3-1.1' && sudo apt-mark hold kubeadm
   57  kubectl get pods
   58  kubeadm --v
   59  kubeadm --version
   60  kubeadm version
   61  kubectl drain master --ignore-daemonset
   62  kubectl drain master --ignore-daemonsets
   63  kubectl drain master --ignore-daemonsets --force
   64  killall -s SIGTERM kube-apiserver && sleep 20
   65  sudo kubeadm upgrade plan
   66  kubeadm version
   67  sudo kubectl upgrade plan
   68  kubectl upgrade plan
   69  kubectl upgrade node
   70  sudo apt-mark unhold kubelet kubectl && sudo apt-get update && sudo apt-get install -y kubelet='1.32.x-*' kubectl='1.32.x-*' && sudo apt-mark hold kubelet kubectl
   71  sudo kubectl upgrade plan
   72  sudo apt-cache madison kubectl
   73  sudo apt-mark unhold kubelet kubectl && sudo apt-get update && sudo apt-get install -y kubelet='1.32.3-1.1' kubectl='1.32.3-1.1' && sudo apt-mark hold kubelet kubectl
   74  kubectl version
   75  kubectl get po
   76  kubectl get po -n kube-system
   77  kubectl get node
   78  kubectl version
   79  sudo systemctl daemon-reload
   80  sudo systemctl restart kubelet
   81  kubectl uncordonmaster
   82  kubectl uncordon master
   83  kubectl get nodes
   84  watch -n 1 kubectl get po -n kube-system
   85  sudo apt-cache madison kubeadm
   86  kubecrl run nginx --image=nginx
   87  kubectl run nginx --image=nginx
   88  watch -n 1 kubectl get po
   89  kubectl run beforebackup --image=nginx
   90  kubectl get pods
   91  etcdctl
   92  apt  install etcd-client
   93  cd /etc/kubernetes/manifests/
   94  ls
   95  nano etcd.yaml 
   96  cat etcd.yml
   97  cat etcd.yaml
   98  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379   --cacert=/etc/kubernetes/pki/etcd/ca.crt   --cert=/etc/kubernetes/pki/etcd/server.crt   --key=/etc/kubernetes/pki/etcd/server.key snapshot save /tmp/snapshot.db
   99  cd /tmp/snap
  100  cd
  101  cd /tmp
  102  ls
  103  kubectl get pods
  104  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379   --cacert=/etc/kubernetes/pki/etcd/ca.crt   --cert=/etc/kubernetes/pki/etcd/server.crt   --key=/etc/kubernetes/pki/etcd/server.key --data-dir=/var/lib/etcd-backup snapshot restore /tmp/snapshot.db
  105  cd 
  106  kubectl get pod
  107  kbectl delete pod --all
  108  kubectl delete pod --all
  109  kubectl run pod --image=nginx
  110  kubectl get pods
  111  kubectl run afterbackup --image=nginx
  112  kubectl get pods
  113  cd /etc/kubernetes/manifests/
  114  ls
  115  nano kube-apiserver.yaml 
  116  nano etcd.yaml 
  117  kubectl get pods -n kube-system
  118  kubectl get po
  119  cd
  120  cd ./kube
  121  cd /etc/var
  122  cd /etc/
  123  ls
  124  kubectl get po
  125  cd
  126  ls -a
  127  cd .kube/
  128  ls
  129  cd 
  130  kubectl get pod --show-labels
  131  kubectl wait --for=condition-ready pod
  132  kubectl get pod --help | grep for
  133  kubectl get pod -ojson | jq'.items[].status'
  134  apt install jq
  135  kubectl get pod -ojson | jq'.items[].status'
  136  kubectl get pod -ojson | 'jq.items[].status'
  137  jq.status
  138  jq --help
  139  kubectl get pod -ojson | jq '.items[].status'
  140  kubectl config view -o jsonpath='{.users[].name}' 
  141  ls
  142  kubectl get pods -o jsonpath="{.items[*].metadata.name}"
  143  kubectl get pods -o jsonpath="{range .items[*]}{.metadata.name}{'\n'}{end}"
  144  kubectl get pods -o json | jq '.items[].metadata.name'
  145  kubectl get pods -o json | jq '.items[] | select(.status.phase=="Running") | .metadata.name'
  146  kubectl get po -o json
  147  kubectl get pods -o jsonpath="{.items[*].metadata.name}"
  148  kubectl run nginx --image=nginx --dry-run=client -o json > pod.json
  149  ls
  150  history

crictl ps | grep kube-proxy
  152  crictl logs 53fa92cba03ce



----------------------------------------------------
24/3/25

helm


hostnamectl set-hostname kmaster
    2  hostname
    3  cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
    4  overlay
    5  br_netfilter
    6  EOF
    7  sudo modprobe overlay
    8  sudo modprobe br_netfilter
    9  cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
   10  net.bridge.bridge-nf-call-iptables = 1
   11  net.bridge.bridge-nf-call-ip6tables = 1
   12  net.ipv4.ip_forward = 1
   13  EOF
   14  sudo sysctl --system
   15  sudo swapoff -a
   16  (crontab -l 2>/dev/null; echo "@reboot /sbin/swapoff -a") | crontab - || true
   17  sudo apt-get update -y
   18  sudo apt-get install -y software-properties-common gpg curl apt-transport-https ca-certificates
   19  curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
   20  echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /" | tee /etc/apt/sources.list.d/cri-o.list
   21  sudo apt-get update -y
   22  sudo apt-get install -y cri-o
   23  sudo systemctl daemon-reload
   24  sudo systemctl enable crio --now
   25  sudo systemctl start crio.service
   26  systemctl status crio
   27  systemctl start crio
   28  VERSION="v1.32.0"
   29  wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz
   30  sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin
   31  rm -f crictl-$VERSION-linux-amd64.tar.gz
   32  KUBERNETES_VERSION=1.32
   33  curl -fsSL https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
   34  echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$KUBERNETES_VERSION/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
   35  apt update -y
   36  sudo apt-get install -y kubelet kubeadm kubectl
   37  kubedm version
   38  kubeadm version
   39  kubelet version
   40  kubectl version
   41  kubeadm init
   42  hostnamectl set-hostname-kmaster
   43  hostnamectl set-hostname kmaster
   44  hostname
   45  kubectl reset
   46  kubeadm reset
   47  kubeadm init
   48  mkdir -p $HOME/.kube
   49  kubectl get nodes
   50  kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
   51  kubectl get nodes
   52  kubectl describe node kmaster
   53  kubectl get nodes
   54  curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
   55  sudo apt-get install apt-transport-https --yes
   56  echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
   57  sudo apt-get update
   58  sudo apt-get install helm
   59  helm create myhelm
   60  tree
   61  apt  install tree
   62  tree myhelm
   63  cd myhelm/
   64  ls
   65  cat Chart.yaml 
   66  helm create my_chart
   67  cd my_chart/
   68  ls
   69  cat Chart.yaml 
   70  cd charts/
   71  ls
   72  cd ..
   73  cd templates/
   74  ls
   75  cat deployment.yaml 
   76  cd ../values.yaml 
   77  cd ..
   78  cat values.yaml 
   79  nano values.yaml 
   80  cd
   81  helm template my_chart
   82  ls
   83  helm template myhelm/
   84  ls
   85  helm template myhelm
   86  helm lint myhelm/
   87  helm install abhichart myhelm/
   88  kubectl get all
   89  cd myhelm/
   90  ls
   91  cd my_chart/
   92  ls
   93  cat values.yaml 
   94  nano values.yaml 
   95  nano Chart.yaml 
   96  kubectl get all
   97  kubectl describe pod pod/abhichart-myhelm-756b847c6-w7qd7
   98  kubectl describe pod abhichart-myhelm-756b847c6-w7qd7
   99  kubectl describe node kmaster | grep -i taint
  100  kubectl taint node kmaster node-role.kubernetes.io/control-plane:NoSchedule-
  101  kubectl get all
  102  ls
  103  cd templates/
  104  ls
  105  cat ingress.yaml 
  106  cd
  107  history
  ---------------------------------------

  cd /etc/kubernetes/manifests/
  156  ls
  157  cat kube-apiserver.yaml | grep range
  158  cd
  159  kubectl get pod -n kube-system
  160  cd /etc/
  161  ls
  162  ls cni*
  163  cd cni
  164  ls
  165  cd net.d/
  166  ls
  167  cat 10-weave.conflist 
  168  kubectl get pod -n kube-system
  169  cd
  170  kubectl get events --all ns --sort-by=.metadata.creationTimestamp
  171  kubectl get events -A --all --sort-by=.metadata.creationTimestamp
  172  kubectl get events -A  --sort-by=.metadata.creationTimestamp
  173  kubectl get pod -n kube-system
  174  kubctl describe pod kube-proxy-h2jm4
  175  kubectl describe pod kube-proxy-h2jm4
  176  kubectl describe pod kube-proxy-h2jm4 -n kube-system
  177  kubectl describe pod kube-proxy-h2jm4 -n kube-system | grep -i eve
  178  kubectl delete pod kube-proxy-h2jm4
  179  kubectl delete pod kube-proxy-h2jm4 -n kube-system
  180  kubectl get events
  181  kubectl get events -n kube-system
  182  kubectl get pods -n kube-system
  183  ps -ef | grep runc
  184  crictl ps | grep kube-proxy
  185  crictl ps -ef| grep kube-proxy
  186  ps -ef | grep -i containerd
  187  kill -9 39513
  188  clear
  189  kubectl create ns cka-master
  190  kubectl get -all
  191  kubectl get -A
  192  kubectl get all -A
  193  kubectl get all -A > abc.txt
  194  kubectl get roles --all-ns
  195  kubectl get roles --all-namespaces
  196  kubectl get roles --all-namespaces -o wide
  197  cat abc.txt 
  198  k api-resources --namespaced -o name 
  199  kubectl api-resources --namespaced -o name 
  200  kubectl api-resources --help
  201  kubectl api-resources --namespaced
  202  kubectl api-resources --namespaced=false
  203  kubectl api-resources --namespaced=false -o name
  204  clear
  205  k get ns 
  206  clear
  207  alias k=kubectl
  208  k get ns
  209  k run tigers-reunite --image=https:2.4.41-alpine --labels="pod=container" --dry-run=client -o yaml
  210  vi abc.yamk
  211  vi tiger.yaml
  212  k apply -f tiger.yaml -n project-tiger
  213  k get pod -n project-tiger
  214  k describe pod tiger.yaml 
  215  k describe pod tigers-reunit
  216  k describe pod tigers-reunite
  217  k describe pod tigers-reunite -n projecr-tiger
  218  k describe pod tigers-reunite -n project-tiger
  219  k edit pod tiger.yaml 
  220  k edit pod tigers-reunite -n project-tiger
  221  k get pod -n project-tiger
  222  clear
  223  k get pod -n project-tiger
  224  k get pod -n project-tiger -o wide
  225  ssh kmaster
  226  crictl ps 
  227  crictl ps | grep tigers-reunite
  228  echo "d7dda0d9d51ba" > container.txt
  229  k logs -c d7dda0d9d51ba
  230  k describe pod tigers-reunite -n project-tiger
  231  k logs tigers-reunite -c tigers-reunite -n project-tiger
  232  k logs tigers-reunite -c tigers-reunite -n project-tiger > klogs.txt
  233  history



